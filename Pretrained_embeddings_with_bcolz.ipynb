{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "bcolz: columnar and compressed data container\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "glove_path = \"glove.6B/glove.6B.50d.txt\"\n",
    "glove_path_out = \"glove.6B/glove.6B.50d.dat\"\n",
    "EMBEDDING_DIM = 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "\n",
    "# File to save the embeddings\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=glove_path_out, mode='w')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "with open(glove_path, encoding='utf-8', mode=\"r\") as f:\n",
    "    for l in f:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "\n",
    "vocab_size = len(words)\n",
    "vectors = bcolz.carray(vectors[1:].reshape((vocab_size, EMBEDDING_DIM)), rootdir=glove_path_out, mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(\"6B.50_words.pkl\",\"wb\"))\n",
    "pickle.dump(word2idx, open(\"6B.50_idx.pk\", \"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now create a dictionary that given a word returns its vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "vectors = bcolz.open(glove_path_out)[:]\n",
    "words = pickle.load(open(\"6B.50_words.pkl\", \"rb\"))\n",
    "word2idx = pickle.load(open(\"6B.50_idx.pk\", \"rb\"))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "print(glove[\"the\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using this dictionary for creating the Pytorch embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 unique words\n"
     ]
    }
   ],
   "source": [
    "my_custom_vocab = [\"xxpad\", \"xxunk\", \"xxkno\",\"the\", \"friend\", \"year\", \"when\"]\n",
    "\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".replace(\".\",\"\").replace(\",\",\"\").lower().split()\n",
    "\n",
    "vocabulary = my_custom_vocab + raw_text\n",
    "vocabulary = set(vocabulary)\n",
    "vocab_size = len(vocabulary)\n",
    "print(\"There are {} unique words\".format(vocab_size))\n",
    "\n",
    "\n",
    "word2id = {w:i for i,w in enumerate(set(vocabulary))}\n",
    "id2word = {v:k for k,v in word2id.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "weights_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "words_found = 0\n",
    "print(weights_matrix.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0;spirits\n",
      "1;pattern\n",
      "2;study\n",
      "3;idea\n",
      "4;with\n",
      "5;process\n",
      "6;xxpad\n",
      "7;the\n",
      "8;manipulate\n",
      "9;beings\n",
      "10;friend\n",
      "11;data\n",
      "12;conjure\n",
      "13;abstract\n",
      "14;evolve\n",
      "15;effect\n",
      "16;computer\n",
      "17;a\n",
      "18;our\n",
      "19;when\n",
      "20;we\n",
      "21;about\n",
      "22;people\n",
      "23;by\n",
      "24;that\n",
      "25;evolution\n",
      "26;direct\n",
      "27;year\n",
      "28;create\n",
      "29;xxkno\n",
      "30;computers\n",
      "31;directed\n",
      "32;program\n",
      "33;they\n",
      "34;other\n",
      "35;xxunk\n",
      "36;programs\n",
      "37;computational\n",
      "38;to\n",
      "39;called\n",
      "40;as\n",
      "41;of\n",
      "42;is\n",
      "43;spells\n",
      "44;inhabit\n",
      "45;are\n",
      "46;processes\n",
      "47;things\n",
      "48;rules\n",
      "49;in\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(vocabulary):\n",
    "    print(\"{};{}\".format(i, word))\n",
    "    try:\n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "        \n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the embedding layer. This could be inside the NN class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, trainable=True):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    #emb_layer.load_state_dict({\"weight\": weights_matrix})\n",
    "    emb_layer.weight.data.copy_(torch.Tensor(weights_matrix))\n",
    "    if not trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "embedding, v_size, emb_dim = create_emb_layer(weights_matrix, trainable=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29])\n",
      "tensor([[ 1.1993,  0.0340, -0.2373, -0.3520, -0.0186,  1.3704,  0.0654,  0.4761,\n",
      "         -0.7745, -0.1020,  0.6647, -0.1310, -0.2796,  0.3668, -0.5716,  1.1656,\n",
      "          0.6293,  0.3306, -0.6955, -0.1583,  1.3894, -0.3072,  1.3048, -0.1306,\n",
      "         -0.9608,  0.4253, -0.6285, -0.6249,  1.4926,  0.4582, -0.3989, -0.3842,\n",
      "         -0.9446,  0.1732, -0.1850,  0.3945,  0.7606,  0.1778, -0.2165,  0.9184,\n",
      "         -0.5465,  0.0604, -0.2687, -0.0682, -0.0783, -0.0518,  0.0919,  0.2362,\n",
      "         -0.6619,  0.9910]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "w_id = torch.LongTensor([word2id[\"xxkno\"]])\n",
    "print(w_id)\n",
    "print(embedding(w_id))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}