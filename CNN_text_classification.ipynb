{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://nbviewer.jupyter.org/github/cezannec/CNN_Text_Classification/blob/master/CNN_Text_Classification.ipynb\n",
    "https://github.com/cezannec/CNN_Text_Classification\n",
    "\n",
    "In this notebook, I'll train a CNN to classify the sentiment of movie reviews in a corpus of text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-11 22:21:22--  https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/labels.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.196.133\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.196.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 225000 (220K) [text/plain]\r\n",
      "Saving to: ‘labels.txt’\r\n",
      "\r\n",
      "labels.txt          100%[===================>] 219.73K   542KB/s    in 0.4s    \r\n",
      "\r\n",
      "2020-05-11 22:21:23 (542 KB/s) - ‘labels.txt’ saved [225000/225000]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#! wget https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/labels.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-11 22:21:25--  https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/reviews.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.196.133\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.196.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 33678267 (32M) [text/plain]\r\n",
      "Saving to: ‘reviews.txt’\r\n",
      "\r\n",
      "reviews.txt         100%[===================>]  32.12M  70.4KB/s    in 5m 52s  \r\n",
      "\r\n",
      "2020-05-11 22:27:19 (93.5 KB/s) - ‘reviews.txt’ saved [33678267/33678267]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#! wget https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/reviews.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load in the data and visualize it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "\n",
      "positive\n",
      "negative\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "with open('reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = f.read()\n",
    "\n",
    "print(reviews[:1000])\n",
    "print()\n",
    "print(labels[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Pre-processing\n",
    "\n",
    "reviews are delimited with newline characters \\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "reviews = reviews.lower()\n",
    "all_text = \"\".join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "#split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n')\n",
    "all_text = \" \".join(reviews_split)\n",
    "\n",
    "# create a list of all words\n",
    "all_words = all_text.split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such']\n"
     ]
    }
   ],
   "source": [
    "print(all_words[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encoding labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 1=positive, 0=negative\n",
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if label == \"positive\" else 0 for label in labels_split])\n",
    "print(encoded_labels[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing Outliers\n",
    "\n",
    "1. Getting rid of extremely long or short reviews; the outliers\n",
    "\n",
    "2. Padding/truncating the remaining data so that we have reviews of the same length."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "# build a dictionary that maps indices to review lengths\n",
    "counts = Counter(all_words)\n",
    "\n",
    "# outlier review stats; counting words in each review\n",
    "review_lens = Counter([len(x.split()) for x in reviews_split])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers 25001\n",
      "Number of reviews after removing outliers: 24855\n"
     ]
    }
   ],
   "source": [
    "#remove any super short reviews and truncate super long reviews.\n",
    "print(\"Number of reviews before removing outliers\", len(reviews_split))\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_split) if len(review.split()) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_split = [reviews_split[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "# remove reviews larger than 1000 words and their labels\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_split) if len(review.split()) <= 1000]\n",
    "reviews_split = [reviews_split[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print(\"Number of reviews after removing outliers:\", len(reviews_split))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using a Pre-trained Embedding layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: d: No such file or directory\r\n",
      "gzip: word2vec_model/GoogleNews-vectors-negative300-SLIM.bin.gz already has .gz suffix -- unchanged\r\n"
     ]
    }
   ],
   "source": [
    "# load a pretrained word2vec model\n",
    "#! gzip d word2vec_model/GoogleNews-vectors-negative300-SLIM.bin.gz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# creating the model\n",
    "embed_lookup = KeyedVectors.load_word2vec_format('word2vec_model/GoogleNews-vectors-negative300-SLIM.bin',\n",
    "                                                 binary=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# store pretrained vocab\n",
    "pretrained_words = []\n",
    "for word in embed_lookup.vocab:\n",
    "    pretrained_words.append(word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocab: 299567\n",
      "Word in Vocab: for\n",
      "Length of embedding 300\n"
     ]
    }
   ],
   "source": [
    "row_idx = 1\n",
    "# get word/embedding in that row\n",
    "word = pretrained_words[row_idx]\n",
    "embedding = embed_lookup[word]\n",
    "\n",
    "print(\"Size of Vocab: {}\".format(len(pretrained_words)))\n",
    "print(\"Word in Vocab: {}\".format(word))\n",
    "print(\"Length of embedding {}\".format(len(embedding)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "for\n",
      "that\n",
      "is\n",
      "on\n"
     ]
    }
   ],
   "source": [
    "# print a few common words\n",
    "for i in range(5):\n",
    "    print(pretrained_words[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'fabulous'\n",
      "Word wonderful, Similarity: 0.761\n",
      "Word fantastic, Similarity: 0.761\n",
      "Word marvelous, Similarity: 0.730\n",
      "Word gorgeous, Similarity: 0.714\n",
      "Word lovely, Similarity: 0.713\n",
      "Word terrific, Similarity: 0.694\n",
      "Word amazing, Similarity: 0.693\n",
      "Word beautiful, Similarity: 0.670\n",
      "Word magnificent, Similarity: 0.667\n",
      "Word splendid, Similarity: 0.645\n"
     ]
    }
   ],
   "source": [
    "# find similar words\n",
    "# Select a word\n",
    "find_similar_to = \"fabulous\"\n",
    "print(\"Similar words to '{}'\".format(find_similar_to))\n",
    "for similar_word in embed_lookup.similar_by_word(find_similar_to):\n",
    "    print(\"Word {}, Similarity: {:.3f}\".format(similar_word[0], similar_word[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenize Reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Convert reviews to tokens\n",
    "def tokenize_all_reviews(embed_lookup, reviews_split):\n",
    "    # split each review into a list of words\n",
    "    reviews_words = [review.split() for review in reviews_split]\n",
    "\n",
    "    tokenized_reviews = []\n",
    "    for review in reviews_words:\n",
    "        ints = []\n",
    "        for word in review:\n",
    "            try:\n",
    "                idx = embed_lookup.vocab[word].index\n",
    "            except:\n",
    "                idx = 0\n",
    "            ints.append(idx)\n",
    "        tokenized_reviews.append(ints)\n",
    "    return tokenized_reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tokenized_reviews = tokenize_all_reviews(embed_lookup, reviews_split)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   \n",
      "140\n",
      "[0, 137, 3, 0, 11620, 3799, 13, 1215, 10, 9, 194, 54, 12, 73, 61, 685, 41, 183, 243, 129, 12, 1663, 119, 72, 0, 9, 2989, 7334, 242, 159, 0, 453, 2, 0, 137, 1239, 19951, 3, 141, 1980, 0, 1898, 55, 3, 1663, 9, 11124, 0, 3857, 6663, 9, 20401, 295, 28, 45, 148, 157, 102, 27, 15452, 1663, 30714, 9, 65172, 0, 9, 844, 737, 47, 6585, 159, 0, 9, 668, 4365, 1003, 0, 27, 295, 56, 4365, 622, 9, 3832, 0, 43, 0, 897, 3187, 907, 0, 5396, 113, 9, 183, 4365, 1009, 3165, 10, 137, 0, 3288, 296, 10314, 4365, 6638, 213, 0, 8810, 40, 0, 116, 1663, 897, 2059, 0, 0, 137, 4365, 830, 2, 124, 2216, 0, 119, 782, 144, 2, 0, 137, 3, 330, 23046, 78, 0, 16915, 2, 13, 85275, 7451]\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "print(reviews_split[0])\n",
    "print(len(reviews_split[0].split()))\n",
    "print(tokenized_reviews[0])\n",
    "print(len(tokenized_reviews[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Padding sequences\n",
    "\n",
    "Your final features array should be a 2D array, with as many rows as there are reviews,\n",
    "and as many columns as the specified seq_length."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def pad_features(tokenized_reviews, seq_length):\n",
    "    # padding at the beginning\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(tokenized_reviews), seq_length), dtype=int)\n",
    "\n",
    "    # for each review\n",
    "    for i, row in enumerate(tokenized_reviews):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0      0      0      0      0]\n",
      " [     0      0      0      0      0]\n",
      " [ 16483     26      0     12 106210]\n",
      " [  1935   1326     12      0   1403]\n",
      " [     0      0      0      0      0]\n",
      " [     0      0      0      0      0]\n",
      " [     0      0      0      0      0]\n",
      " [     0      0      0      0      0]\n",
      " [     0      0      0      0      0]\n",
      " [    56   4365      8    270    119]\n",
      " [     0      0      0      0      0]\n",
      " [     0      0      0      0      0]\n",
      " [     0      0      0      0      0]\n",
      " [     9    104   1428     16      0]\n",
      " [     0     25  13619  11902   7445]\n",
      " [     0      0      0      0      0]\n",
      " [     9    208  18994  66850 121241]\n",
      " [     0      0      0      0      0]\n",
      " [    38    165  66850 121241  13241]\n",
      " [     9    661      3    675     67]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "\n",
    "features = pad_features(tokenized_reviews, seq_length=seq_length)\n",
    "\n",
    "# Test\n",
    "assert len(features)==len(tokenized_reviews), \"Features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 5 values for the first 20\n",
    "print(features[:20, :5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training, validation and test data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(19884, 200) \n",
      "Validation set: \t(2485, 200) \n",
      "Test set: \t\t(2486, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataloaders and Batching"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# shuffling and batching data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentiment Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Training on CPU\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"Training on GPU\")\n",
    "else:\n",
    "    print(\"No GPU available. Training on CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class SentimentCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim, num_filters=100,\n",
    "                 kernel_sizes=[3,4,5], freeze_embeddings=True, drop_probab=0.5):\n",
    "        super(SentimentCNN, self).__init__()\n",
    "        self.num_filters = num_filters\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        #1. embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors))\n",
    "        if freeze_embeddings:\n",
    "            self.embedding.requires_grad = False\n",
    "\n",
    "        #2. CNN Layers\n",
    "        self.convs_1d = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2, 0))\n",
    "            for k in kernel_sizes])\n",
    "\n",
    "        #3. FC layer for classification\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size)\n",
    "\n",
    "        #4. Dropout and sigmoid layers\n",
    "        self.dropout = nn.Dropout(drop_probab)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"Convolutional + max pooling layer\"\n",
    "        # Squeeze the last dimension to get size: (batch_size, num_filters, conv_seq_length)\n",
    "        # conv_seq_length will be ~ 200\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "\n",
    "        # 1D pool over conv_seq_length\n",
    "        # squeeze to get size: (batch_size, num_filters)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "\n",
    "        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n",
    "\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        logit = self.fc(x)\n",
    "\n",
    "        return self.sig(logit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiate the network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentCNN(\n",
      "  (embedding): Embedding(299567, 300)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 1 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 300-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(net, train_loader, epochs, print_every=100):\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0 # for printing\n",
    "\n",
    "    # train for some number of epochs\n",
    "    net.train()\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output = net(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output = net(inputs)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Step: 100... Loss: 0.459185... Val Loss: 0.440422\n",
      "Epoch: 1/2... Step: 200... Loss: 0.313580... Val Loss: 0.358592\n",
      "Epoch: 1/2... Step: 300... Loss: 0.181484... Val Loss: 0.342895\n",
      "Epoch: 2/2... Step: 400... Loss: 0.149747... Val Loss: 0.325604\n",
      "Epoch: 2/2... Step: 500... Loss: 0.348554... Val Loss: 0.335780\n",
      "Epoch: 2/2... Step: 600... Loss: 0.205872... Val Loss: 0.344993\n",
      "Epoch: 2/2... Step: 700... Loss: 0.422730... Val Loss: 0.347651\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 2 # this is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 100\n",
    "\n",
    "train(net, train_loader, epochs, print_every=print_every)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.386\n",
      "Test accuracy: 0.844\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "    # get predicted outputs\n",
    "    output = net(inputs)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inference on a test review"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "#from string import punctuation\n",
    "\n",
    "# helper function to process and tokenize a single review\n",
    "def tokenize_review(embed_lookup, test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    tokenized_review = []\n",
    "    for word in test_words:\n",
    "        try:\n",
    "            idx = embed_lookup.vocab[word].index\n",
    "        except:\n",
    "            idx = 0\n",
    "        tokenized_review.append(idx)\n",
    "\n",
    "    return tokenized_review"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def predict(embed_lookup, net, test_review, sequence_length=200):\n",
    "    \"\"\"\n",
    "    Predict whether a given test_review has negative or positive sentiment.\n",
    "    \"\"\"\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(embed_lookup, test_review)\n",
    "\n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features([test_ints], seq_length)\n",
    "\n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "\n",
    "    batch_size = feature_tensor.size(0)\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "\n",
    "    # get the output from the model\n",
    "    output = net(feature_tensor)\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())\n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "\n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test on positive/negative reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "seq_length=200 # good to use the length that was trained on"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.000704\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
    "\n",
    "# test negative review\n",
    "predict(embed_lookup, net, test_review_neg, seq_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.999048\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
    "\n",
    "predict(embed_lookup, net, test_review_pos, seq_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}