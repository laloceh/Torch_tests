{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nhttps://buomsoo-kim.github.io/attention/2020/03/26/Attention-mechanism-16.md/\\n\\nsentiment scores: \\nvery positive = 5 \\nslightly positive = 4 \\nneutral = 3\\nslightly negative = 2 \\nvery negative = 1\\n\\n'"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://buomsoo-kim.github.io/attention/2020/03/26/Attention-mechanism-16.md/\n",
    "\n",
    "sentiment scores:\n",
    "very positive = 5\n",
    "slightly positive = 4\n",
    "neutral = 3\n",
    "slightly negative = 2\n",
    "very negative = 1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
      "0  724227031     True      golden                 236               NaN   \n",
      "1  724227032     True      golden                 231               NaN   \n",
      "2  724227033     True      golden                 233               NaN   \n",
      "3  724227034     True      golden                 240               NaN   \n",
      "4  724227035     True      golden                 240               NaN   \n",
      "\n",
      "  sentiment  sentiment:confidence  our_id sentiment_gold  \\\n",
      "0         5                0.7579   10001           5\\n4   \n",
      "1         5                0.8775   10002           5\\n4   \n",
      "2         2                0.6805   10003           2\\n1   \n",
      "3         2                0.8820   10004           2\\n1   \n",
      "4         3                1.0000   10005              3   \n",
      "\n",
      "                               sentiment_gold_reason  \\\n",
      "0  Author is excited about the development of the...   \n",
      "1  Author is excited that driverless cars will be...   \n",
      "2  The author is skeptical of the safety and reli...   \n",
      "3    The author is skeptical of the project's value.   \n",
      "4  Author is making an observation without expres...   \n",
      "\n",
      "                                                text  \n",
      "0  Two places I'd invest all my money if I could:...  \n",
      "1  Awesome! Google driverless cars will help the ...  \n",
      "2  If Google maps can't keep up with road constru...  \n",
      "3  Autonomous cars seem way overhyped given the t...  \n",
      "4  Just saw Google self-driving car on I-34. It w...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Twitter-sentiment-self-drive-DFE.csv\", encoding = 'latin-1')\n",
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 10509.95it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_INSTANCES = 3000\n",
    "MAX_SENT_LEN = 10\n",
    "tweets = []\n",
    "sent_scores = []\n",
    "unique_tokens = set()\n",
    "\n",
    "for i in tqdm(range(NUM_INSTANCES)):\n",
    "    rand_idx = np.random.randint(len(data))\n",
    "\n",
    "    tweet = []\n",
    "    sentences = data['text'].iloc[rand_idx].split(\".\")\n",
    "    for sent in sentences:\n",
    "        if len(sent) != 0:\n",
    "            # Get only words\n",
    "            sent = [x.lower() for x in re.findall(r\"\\w+\", sent)]\n",
    "            if len(sent) >= MAX_SENT_LEN:\n",
    "                sent = sent[:MAX_SENT_LEN]\n",
    "            else:\n",
    "                for _ in range(MAX_SENT_LEN - len(sent)):\n",
    "                    sent.append(\"<pad>\")\n",
    "\n",
    "            tweet.append(sent)\n",
    "            unique_tokens.update(sent)\n",
    "    tweets.append(tweet)\n",
    "    if data['sentiment'].iloc[rand_idx] == \"not_relevant\":\n",
    "        sent_scores.append(0)\n",
    "    else:\n",
    "        sent_scores.append(int(data[\"sentiment\"].iloc[rand_idx]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "[['looks', 'like', 'the', 'google', 'self', 'driving', 'car', 'decided', 'to', 'drive'], ['pic', 'http', 't', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['co', 'xyssrdazge', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets))\n",
    "tweet_id = 14\n",
    "print(tweets[tweet_id])\n",
    "print(len(tweets[tweet_id]))\n",
    "print(sent_scores[tweet_id])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unique tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6275\n",
      "['reasons', 'gladly', 'cbm', 'simpsons', 'wheel', 'wonder', 'brace', 'zl0mcvqzn5', 'n9cssuhgai', 'type', 'niche', 'y1cz3b9aul', 'technewsdaily', 'robin', 'whump', 'clarionledger', 'kickstarter', 'drudge_report', 'arduino', 'piece']\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = list(unique_tokens)\n",
    "print(len(unique_tokens))\n",
    "print(unique_tokens[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Numericalize each token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:03<00:00, 874.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# encode each token into index\n",
    "for i in tqdm(range(len(tweets))):\n",
    "#for i in range(len(tweets)):\n",
    "    for j in range(len(tweets[i])):\n",
    "        tweets[i][j] = [unique_tokens.index(x) for x in tweets[i][j]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5931, 3665, 6075, 3459, 3668, 777, 2717, 4333, 865, 5950], [324, 893, 2970, 4182, 4182, 4182, 4182, 4182, 4182, 4182], [3244, 4370, 4182, 4182, 4182, 4182, 4182, 4182, 4182, 4182]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(tweets[tweet_id])\n",
    "print(len(tweets[tweet_id]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(unique_tokens)\n",
    "NUM_CLASSES = len(set(sent_scores))\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 1#0\n",
    "HIDDEN_SIZE = 16\n",
    "EMBEDDING_DIM = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class wordEncoder(nn.Module):\n",
    "  def __init__(self, vocab_size, hidden_size, embedding_dim):\n",
    "    super(wordEncoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.vocab_size = vocab_size\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = nn.GRU(embedding_dim, hidden_size, bidirectional = True)\n",
    "\n",
    "  def forward(self, word, h0):\n",
    "    word = self.embedding(word).unsqueeze(0).unsqueeze(1)\n",
    "    out, h0 = self.gru(word, h0)\n",
    "    return out, h0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f8e1fab4be0>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class sentEncoder(nn.Module):\n",
    "  def __init__(self, hidden_size):\n",
    "    super(sentEncoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, bidirectional = True)\n",
    "\n",
    "  def forward(self, sentence, h0):\n",
    "    sentence = sentence.unsqueeze(0).unsqueeze(1)\n",
    "    out, h0 = self.gru(sentence)\n",
    "    return out, h0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hierarchical Attention Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "  def __init__(self, wordEncoder, sentEncoder, num_classes, device):\n",
    "    super(HAN, self).__init__()\n",
    "    self.wordEncoder = wordEncoder\n",
    "    self.sentEncoder = sentEncoder\n",
    "    self.device = device\n",
    "    #self.softmax = nn.Softmax(dim=1)\n",
    "    self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    # word-level attention\n",
    "    self.word_attention = nn.Linear(self.wordEncoder.hidden_size*2, self.wordEncoder.hidden_size*2)\n",
    "    self.u_w = nn.Linear(self.wordEncoder.hidden_size*2, 1, bias = False)\n",
    "\n",
    "    # sentence-level attention\n",
    "    self.sent_attention = nn.Linear(self.sentEncoder.hidden_size * 2, self.sentEncoder.hidden_size*2)\n",
    "    self.u_s = nn.Linear(self.sentEncoder.hidden_size*2, 1, bias = False)\n",
    "\n",
    "    # final layer\n",
    "    self.dense_out = nn.Linear(self.sentEncoder.hidden_size*2, num_classes)\n",
    "    self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "\n",
    "  def forward(self, document):\n",
    "    word_attention_weights = []\n",
    "    sentenc_out = torch.zeros((document.size(0), 2, self.sentEncoder.hidden_size)).to(self.device)\n",
    "    # iterate on sentences\n",
    "    h0_sent = torch.zeros(2, 1, self.sentEncoder.hidden_size, dtype = torch.float).to(self.device)\n",
    "    for i in range(document.size(0)):\n",
    "      sent = document[i]\n",
    "      wordenc_out = torch.zeros((sent.size(0), 2, self.wordEncoder.hidden_size)).to(self.device)\n",
    "      h0_word = torch.zeros(2, 1, self.wordEncoder.hidden_size, dtype = torch.float).to(self.device)\n",
    "      # iterate on words\n",
    "      for j in range(sent.size(0)):\n",
    "        _, h0_word = self.wordEncoder(sent[j], h0_word)\n",
    "        wordenc_out[j] = h0_word.squeeze()\n",
    "      #print(wordenc_out)\n",
    "      wordenc_out = wordenc_out.view(wordenc_out.size(0), -1)\n",
    "      u_word = torch.tanh(self.word_attention(wordenc_out))\n",
    "      #print()\n",
    "      #print(\"u_word\")\n",
    "      #print(u_word)\n",
    "\n",
    "      x = self.u_w(u_word)\n",
    "      #print(\"~~~~~ x ~~~~~\")\n",
    "      #print(x)\n",
    "      #print(x.shape)\n",
    "      #word_weights = self.softmax(self.u_w(u_word))\n",
    "      #aligned_weights_ = F.softmax(aligned_weights.unsqueeze(0))\n",
    "      word_weights = self.softmax(x)\n",
    "\n",
    "      #print()\n",
    "      #print(\"word_weights\")\n",
    "      #print(word_weights)\n",
    "      word_attention_weights.append(word_weights)\n",
    "\n",
    "      sent_summ_vector = (u_word * word_weights).sum(axis=0)\n",
    "\n",
    "      _, h0_sent = self.sentEncoder(sent_summ_vector, h0_sent)\n",
    "      sentenc_out[i] = h0_sent.squeeze()\n",
    "    sentenc_out = sentenc_out.view(sentenc_out.size(0), -1)\n",
    "    u_sent = torch.tanh(self.sent_attention(sentenc_out))\n",
    "    sent_weights = self.softmax(self.u_s(u_sent))\n",
    "    doc_summ_vector = (u_sent * sent_weights).sum(axis=0)\n",
    "    out = self.dense_out(doc_summ_vector)\n",
    "    return word_attention_weights, sent_weights, self.log_softmax(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word_weights\n",
      "tensor([[0.0861],\n",
      "        [0.0833],\n",
      "        [0.1047],\n",
      "        [0.1055],\n",
      "        [0.1011],\n",
      "        [0.1135],\n",
      "        [0.1072],\n",
      "        [0.1005],\n",
      "        [0.0989],\n",
      "        [0.0993]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1048],\n",
      "        [0.1181],\n",
      "        [0.1057],\n",
      "        [0.1000],\n",
      "        [0.0972],\n",
      "        [0.0958],\n",
      "        [0.0951],\n",
      "        [0.0946],\n",
      "        [0.0944],\n",
      "        [0.0943]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0840],\n",
      "        [0.1063],\n",
      "        [0.0999],\n",
      "        [0.0965],\n",
      "        [0.1111],\n",
      "        [0.1059],\n",
      "        [0.1007],\n",
      "        [0.1086],\n",
      "        [0.1059],\n",
      "        [0.0812]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1049],\n",
      "        [0.1004],\n",
      "        [0.0984],\n",
      "        [0.0981],\n",
      "        [0.0985],\n",
      "        [0.0991],\n",
      "        [0.0996],\n",
      "        [0.1000],\n",
      "        [0.1004],\n",
      "        [0.1006]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0985],\n",
      "        [0.0812],\n",
      "        [0.0785],\n",
      "        [0.0997],\n",
      "        [0.1110],\n",
      "        [0.1207],\n",
      "        [0.1077],\n",
      "        [0.1094],\n",
      "        [0.0988],\n",
      "        [0.0945]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1011],\n",
      "        [0.0987],\n",
      "        [0.0938],\n",
      "        [0.0956],\n",
      "        [0.0978],\n",
      "        [0.0989],\n",
      "        [0.0953],\n",
      "        [0.1077],\n",
      "        [0.1039],\n",
      "        [0.1071]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0980],\n",
      "        [0.0830],\n",
      "        [0.0813],\n",
      "        [0.0939],\n",
      "        [0.1152],\n",
      "        [0.1031],\n",
      "        [0.1116],\n",
      "        [0.1105],\n",
      "        [0.1065],\n",
      "        [0.0969]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1054],\n",
      "        [0.0976],\n",
      "        [0.1159],\n",
      "        [0.0981],\n",
      "        [0.0924],\n",
      "        [0.1050],\n",
      "        [0.0986],\n",
      "        [0.0963],\n",
      "        [0.0954],\n",
      "        [0.0952]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1056],\n",
      "        [0.1058],\n",
      "        [0.1013],\n",
      "        [0.0992],\n",
      "        [0.0982],\n",
      "        [0.0978],\n",
      "        [0.0978],\n",
      "        [0.0979],\n",
      "        [0.0981],\n",
      "        [0.0983]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1050],\n",
      "        [0.1053],\n",
      "        [0.1010],\n",
      "        [0.0992],\n",
      "        [0.0984],\n",
      "        [0.0982],\n",
      "        [0.0981],\n",
      "        [0.0982],\n",
      "        [0.0983],\n",
      "        [0.0984]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1042],\n",
      "        [0.1013],\n",
      "        [0.0955],\n",
      "        [0.0998],\n",
      "        [0.0876],\n",
      "        [0.1079],\n",
      "        [0.1041],\n",
      "        [0.1008],\n",
      "        [0.0967],\n",
      "        [0.1021]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0992],\n",
      "        [0.1028],\n",
      "        [0.0953],\n",
      "        [0.0930],\n",
      "        [0.0958],\n",
      "        [0.1151],\n",
      "        [0.1112],\n",
      "        [0.1002],\n",
      "        [0.0964],\n",
      "        [0.0910]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1054],\n",
      "        [0.1106],\n",
      "        [0.1012],\n",
      "        [0.0982],\n",
      "        [0.0972],\n",
      "        [0.0970],\n",
      "        [0.0972],\n",
      "        [0.0975],\n",
      "        [0.0978],\n",
      "        [0.0980]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0804],\n",
      "        [0.0779],\n",
      "        [0.0967],\n",
      "        [0.0941],\n",
      "        [0.1133],\n",
      "        [0.1207],\n",
      "        [0.1152],\n",
      "        [0.1059],\n",
      "        [0.0964],\n",
      "        [0.0994]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1032],\n",
      "        [0.1131],\n",
      "        [0.0990],\n",
      "        [0.1157],\n",
      "        [0.0932],\n",
      "        [0.0872],\n",
      "        [0.0985],\n",
      "        [0.1053],\n",
      "        [0.0924],\n",
      "        [0.0924]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0988],\n",
      "        [0.0889],\n",
      "        [0.0854],\n",
      "        [0.1067],\n",
      "        [0.1143],\n",
      "        [0.1136],\n",
      "        [0.1052],\n",
      "        [0.1016],\n",
      "        [0.0936],\n",
      "        [0.0919]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1054],\n",
      "        [0.1012],\n",
      "        [0.0985],\n",
      "        [0.0983],\n",
      "        [0.0986],\n",
      "        [0.0990],\n",
      "        [0.0994],\n",
      "        [0.0997],\n",
      "        [0.0999],\n",
      "        [0.1001]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0980],\n",
      "        [0.0986],\n",
      "        [0.1042],\n",
      "        [0.0994],\n",
      "        [0.0969],\n",
      "        [0.1051],\n",
      "        [0.1132],\n",
      "        [0.1086],\n",
      "        [0.0900],\n",
      "        [0.0861]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1165],\n",
      "        [0.1095],\n",
      "        [0.1007],\n",
      "        [0.0977],\n",
      "        [0.0962],\n",
      "        [0.0957],\n",
      "        [0.0957],\n",
      "        [0.0958],\n",
      "        [0.0960],\n",
      "        [0.0962]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1019],\n",
      "        [0.1097],\n",
      "        [0.1069],\n",
      "        [0.0888],\n",
      "        [0.0967],\n",
      "        [0.1004],\n",
      "        [0.1056],\n",
      "        [0.0981],\n",
      "        [0.0963],\n",
      "        [0.0954]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1002],\n",
      "        [0.1049],\n",
      "        [0.1067],\n",
      "        [0.1219],\n",
      "        [0.0950],\n",
      "        [0.0871],\n",
      "        [0.0971],\n",
      "        [0.0921],\n",
      "        [0.0980],\n",
      "        [0.0971]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1060],\n",
      "        [0.1099],\n",
      "        [0.0970],\n",
      "        [0.1081],\n",
      "        [0.0960],\n",
      "        [0.0974],\n",
      "        [0.0930],\n",
      "        [0.1040],\n",
      "        [0.0960],\n",
      "        [0.0927]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1060],\n",
      "        [0.1136],\n",
      "        [0.0983],\n",
      "        [0.1005],\n",
      "        [0.1052],\n",
      "        [0.0994],\n",
      "        [0.0976],\n",
      "        [0.0943],\n",
      "        [0.0929],\n",
      "        [0.0923]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0964],\n",
      "        [0.1201],\n",
      "        [0.1084],\n",
      "        [0.1019],\n",
      "        [0.0984],\n",
      "        [0.0965],\n",
      "        [0.0954],\n",
      "        [0.0947],\n",
      "        [0.0942],\n",
      "        [0.0939]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1033],\n",
      "        [0.0953],\n",
      "        [0.1167],\n",
      "        [0.1016],\n",
      "        [0.1102],\n",
      "        [0.1083],\n",
      "        [0.0902],\n",
      "        [0.0862],\n",
      "        [0.0959],\n",
      "        [0.0922]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1030],\n",
      "        [0.1065],\n",
      "        [0.1011],\n",
      "        [0.0990],\n",
      "        [0.0983],\n",
      "        [0.0981],\n",
      "        [0.0982],\n",
      "        [0.0984],\n",
      "        [0.0986],\n",
      "        [0.0988]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1044],\n",
      "        [0.0962],\n",
      "        [0.1027],\n",
      "        [0.1017],\n",
      "        [0.0948],\n",
      "        [0.0995],\n",
      "        [0.0919],\n",
      "        [0.1141],\n",
      "        [0.0985],\n",
      "        [0.0963]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0947],\n",
      "        [0.0878],\n",
      "        [0.0902],\n",
      "        [0.1050],\n",
      "        [0.1143],\n",
      "        [0.1118],\n",
      "        [0.1115],\n",
      "        [0.1040],\n",
      "        [0.0927],\n",
      "        [0.0880]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1065],\n",
      "        [0.1065],\n",
      "        [0.0964],\n",
      "        [0.1026],\n",
      "        [0.0902],\n",
      "        [0.0873],\n",
      "        [0.0968],\n",
      "        [0.1036],\n",
      "        [0.1013],\n",
      "        [0.1089]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1147],\n",
      "        [0.1052],\n",
      "        [0.1026],\n",
      "        [0.0991],\n",
      "        [0.0972],\n",
      "        [0.0963],\n",
      "        [0.0961],\n",
      "        [0.0961],\n",
      "        [0.0963],\n",
      "        [0.0965]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1021],\n",
      "        [0.1053],\n",
      "        [0.1012],\n",
      "        [0.0997],\n",
      "        [0.0989],\n",
      "        [0.0986],\n",
      "        [0.0985],\n",
      "        [0.0985],\n",
      "        [0.0986],\n",
      "        [0.0986]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0927],\n",
      "        [0.1093],\n",
      "        [0.1066],\n",
      "        [0.1063],\n",
      "        [0.0886],\n",
      "        [0.0915],\n",
      "        [0.1060],\n",
      "        [0.1088],\n",
      "        [0.0965],\n",
      "        [0.0938]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0974],\n",
      "        [0.0977],\n",
      "        [0.1082],\n",
      "        [0.1165],\n",
      "        [0.1034],\n",
      "        [0.0981],\n",
      "        [0.0957],\n",
      "        [0.0947],\n",
      "        [0.0942],\n",
      "        [0.0941]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1008],\n",
      "        [0.0911],\n",
      "        [0.1036],\n",
      "        [0.0982],\n",
      "        [0.1098],\n",
      "        [0.1070],\n",
      "        [0.0935],\n",
      "        [0.1036],\n",
      "        [0.0951],\n",
      "        [0.0973]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1060],\n",
      "        [0.1023],\n",
      "        [0.0993],\n",
      "        [0.0984],\n",
      "        [0.0983],\n",
      "        [0.0986],\n",
      "        [0.0989],\n",
      "        [0.0991],\n",
      "        [0.0994],\n",
      "        [0.0996]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1070],\n",
      "        [0.1007],\n",
      "        [0.1052],\n",
      "        [0.0888],\n",
      "        [0.0843],\n",
      "        [0.1054],\n",
      "        [0.1040],\n",
      "        [0.1025],\n",
      "        [0.1014],\n",
      "        [0.1007]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0898],\n",
      "        [0.1143],\n",
      "        [0.1012],\n",
      "        [0.1013],\n",
      "        [0.0923],\n",
      "        [0.0964],\n",
      "        [0.0937],\n",
      "        [0.1026],\n",
      "        [0.1034],\n",
      "        [0.1050]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1060],\n",
      "        [0.0988],\n",
      "        [0.1181],\n",
      "        [0.1028],\n",
      "        [0.1035],\n",
      "        [0.0967],\n",
      "        [0.0944],\n",
      "        [0.0935],\n",
      "        [0.0932],\n",
      "        [0.0931]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0903],\n",
      "        [0.0963],\n",
      "        [0.1102],\n",
      "        [0.0991],\n",
      "        [0.1029],\n",
      "        [0.0958],\n",
      "        [0.0946],\n",
      "        [0.1056],\n",
      "        [0.1015],\n",
      "        [0.1038]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0976],\n",
      "        [0.0819],\n",
      "        [0.0974],\n",
      "        [0.1004],\n",
      "        [0.0967],\n",
      "        [0.1056],\n",
      "        [0.1190],\n",
      "        [0.0998],\n",
      "        [0.1058],\n",
      "        [0.0959]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0971],\n",
      "        [0.0957],\n",
      "        [0.1064],\n",
      "        [0.1110],\n",
      "        [0.1060],\n",
      "        [0.1038],\n",
      "        [0.1058],\n",
      "        [0.0953],\n",
      "        [0.0875],\n",
      "        [0.0913]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1097],\n",
      "        [0.1019],\n",
      "        [0.0990],\n",
      "        [0.0981],\n",
      "        [0.0980],\n",
      "        [0.0981],\n",
      "        [0.0984],\n",
      "        [0.0987],\n",
      "        [0.0989],\n",
      "        [0.0991]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1068],\n",
      "        [0.1025],\n",
      "        [0.0943],\n",
      "        [0.0912],\n",
      "        [0.0989],\n",
      "        [0.0995],\n",
      "        [0.1093],\n",
      "        [0.1012],\n",
      "        [0.0988],\n",
      "        [0.0975]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1006],\n",
      "        [0.1006],\n",
      "        [0.1000],\n",
      "        [0.0999],\n",
      "        [0.0998],\n",
      "        [0.0998],\n",
      "        [0.0998],\n",
      "        [0.0998],\n",
      "        [0.0998],\n",
      "        [0.0999]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1125],\n",
      "        [0.0985],\n",
      "        [0.0958],\n",
      "        [0.1035],\n",
      "        [0.0983],\n",
      "        [0.0947],\n",
      "        [0.0960],\n",
      "        [0.0984],\n",
      "        [0.1005],\n",
      "        [0.1018]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0966],\n",
      "        [0.0981],\n",
      "        [0.0963],\n",
      "        [0.1030],\n",
      "        [0.0957],\n",
      "        [0.0924],\n",
      "        [0.0963],\n",
      "        [0.1075],\n",
      "        [0.1170],\n",
      "        [0.0972]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0892],\n",
      "        [0.0979],\n",
      "        [0.0974],\n",
      "        [0.1086],\n",
      "        [0.0962],\n",
      "        [0.0929],\n",
      "        [0.1058],\n",
      "        [0.1088],\n",
      "        [0.1030],\n",
      "        [0.1003]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1083],\n",
      "        [0.0936],\n",
      "        [0.0883],\n",
      "        [0.0798],\n",
      "        [0.0815],\n",
      "        [0.0998],\n",
      "        [0.1048],\n",
      "        [0.1259],\n",
      "        [0.1122],\n",
      "        [0.1059]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0970],\n",
      "        [0.0943],\n",
      "        [0.0899],\n",
      "        [0.1198],\n",
      "        [0.1021],\n",
      "        [0.0953],\n",
      "        [0.1018],\n",
      "        [0.1005],\n",
      "        [0.0997],\n",
      "        [0.0997]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1003],\n",
      "        [0.1024],\n",
      "        [0.0996],\n",
      "        [0.0973],\n",
      "        [0.0981],\n",
      "        [0.0988],\n",
      "        [0.0997],\n",
      "        [0.1006],\n",
      "        [0.1013],\n",
      "        [0.1019]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0899],\n",
      "        [0.1104],\n",
      "        [0.0974],\n",
      "        [0.1069],\n",
      "        [0.1171],\n",
      "        [0.1049],\n",
      "        [0.0964],\n",
      "        [0.0932],\n",
      "        [0.0921],\n",
      "        [0.0917]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1121],\n",
      "        [0.1002],\n",
      "        [0.1084],\n",
      "        [0.0986],\n",
      "        [0.1070],\n",
      "        [0.0976],\n",
      "        [0.0948],\n",
      "        [0.0938],\n",
      "        [0.0937],\n",
      "        [0.0938]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0914],\n",
      "        [0.1074],\n",
      "        [0.1123],\n",
      "        [0.1043],\n",
      "        [0.0987],\n",
      "        [0.1071],\n",
      "        [0.0987],\n",
      "        [0.0971],\n",
      "        [0.0924],\n",
      "        [0.0906]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0978],\n",
      "        [0.1057],\n",
      "        [0.1054],\n",
      "        [0.1144],\n",
      "        [0.0973],\n",
      "        [0.0980],\n",
      "        [0.0980],\n",
      "        [0.0975],\n",
      "        [0.0963],\n",
      "        [0.0896]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0896],\n",
      "        [0.0972],\n",
      "        [0.0985],\n",
      "        [0.0964],\n",
      "        [0.1040],\n",
      "        [0.1057],\n",
      "        [0.1038],\n",
      "        [0.1054],\n",
      "        [0.1099],\n",
      "        [0.0895]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0942],\n",
      "        [0.1127],\n",
      "        [0.1037],\n",
      "        [0.0990],\n",
      "        [0.0988],\n",
      "        [0.0981],\n",
      "        [0.0980],\n",
      "        [0.0982],\n",
      "        [0.0985],\n",
      "        [0.0988]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0979],\n",
      "        [0.1091],\n",
      "        [0.1020],\n",
      "        [0.0990],\n",
      "        [0.0981],\n",
      "        [0.0981],\n",
      "        [0.0984],\n",
      "        [0.0988],\n",
      "        [0.0991],\n",
      "        [0.0995]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0850],\n",
      "        [0.0911],\n",
      "        [0.1057],\n",
      "        [0.1029],\n",
      "        [0.1117],\n",
      "        [0.1063],\n",
      "        [0.1020],\n",
      "        [0.0996],\n",
      "        [0.0983],\n",
      "        [0.0975]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0886],\n",
      "        [0.0913],\n",
      "        [0.0945],\n",
      "        [0.1001],\n",
      "        [0.1108],\n",
      "        [0.1026],\n",
      "        [0.0935],\n",
      "        [0.0997],\n",
      "        [0.1039],\n",
      "        [0.1149]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0975],\n",
      "        [0.0956],\n",
      "        [0.0989],\n",
      "        [0.0995],\n",
      "        [0.1001],\n",
      "        [0.1007],\n",
      "        [0.1013],\n",
      "        [0.1018],\n",
      "        [0.1022],\n",
      "        [0.1025]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0979],\n",
      "        [0.0995],\n",
      "        [0.0997],\n",
      "        [0.0995],\n",
      "        [0.0997],\n",
      "        [0.1001],\n",
      "        [0.1004],\n",
      "        [0.1008],\n",
      "        [0.1011],\n",
      "        [0.1013]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0923],\n",
      "        [0.1015],\n",
      "        [0.0933],\n",
      "        [0.0953],\n",
      "        [0.0986],\n",
      "        [0.1088],\n",
      "        [0.0954],\n",
      "        [0.0925],\n",
      "        [0.1093],\n",
      "        [0.1129]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0913],\n",
      "        [0.0995],\n",
      "        [0.1032],\n",
      "        [0.0980],\n",
      "        [0.0962],\n",
      "        [0.1002],\n",
      "        [0.1015],\n",
      "        [0.1025],\n",
      "        [0.1034],\n",
      "        [0.1041]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0938],\n",
      "        [0.1082],\n",
      "        [0.1016],\n",
      "        [0.0994],\n",
      "        [0.0989],\n",
      "        [0.0991],\n",
      "        [0.0994],\n",
      "        [0.0997],\n",
      "        [0.0999],\n",
      "        [0.1001]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1135],\n",
      "        [0.1057],\n",
      "        [0.1019],\n",
      "        [0.0922],\n",
      "        [0.1038],\n",
      "        [0.0993],\n",
      "        [0.0857],\n",
      "        [0.0988],\n",
      "        [0.1083],\n",
      "        [0.0909]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1020],\n",
      "        [0.1020],\n",
      "        [0.0870],\n",
      "        [0.0945],\n",
      "        [0.0979],\n",
      "        [0.1003],\n",
      "        [0.1022],\n",
      "        [0.1036],\n",
      "        [0.1048],\n",
      "        [0.1056]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0858],\n",
      "        [0.0914],\n",
      "        [0.0938],\n",
      "        [0.1076],\n",
      "        [0.1050],\n",
      "        [0.1024],\n",
      "        [0.1032],\n",
      "        [0.0982],\n",
      "        [0.1008],\n",
      "        [0.1118]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0997],\n",
      "        [0.1030],\n",
      "        [0.1093],\n",
      "        [0.1011],\n",
      "        [0.0961],\n",
      "        [0.0906],\n",
      "        [0.0943],\n",
      "        [0.0941],\n",
      "        [0.1075],\n",
      "        [0.1044]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0922],\n",
      "        [0.0890],\n",
      "        [0.0970],\n",
      "        [0.0997],\n",
      "        [0.1014],\n",
      "        [0.1027],\n",
      "        [0.1036],\n",
      "        [0.1043],\n",
      "        [0.1048],\n",
      "        [0.1052]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0852],\n",
      "        [0.1016],\n",
      "        [0.1111],\n",
      "        [0.1055],\n",
      "        [0.1017],\n",
      "        [0.0999],\n",
      "        [0.0992],\n",
      "        [0.0988],\n",
      "        [0.0986],\n",
      "        [0.0984]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0984],\n",
      "        [0.1057],\n",
      "        [0.1130],\n",
      "        [0.1084],\n",
      "        [0.0914],\n",
      "        [0.0966],\n",
      "        [0.0924],\n",
      "        [0.0827],\n",
      "        [0.1022],\n",
      "        [0.1093]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0909],\n",
      "        [0.0874],\n",
      "        [0.0965],\n",
      "        [0.0998],\n",
      "        [0.1018],\n",
      "        [0.1032],\n",
      "        [0.1042],\n",
      "        [0.1049],\n",
      "        [0.1055],\n",
      "        [0.1059]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0914],\n",
      "        [0.1043],\n",
      "        [0.0961],\n",
      "        [0.0919],\n",
      "        [0.0990],\n",
      "        [0.1013],\n",
      "        [0.1027],\n",
      "        [0.1037],\n",
      "        [0.1045],\n",
      "        [0.1050]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0883],\n",
      "        [0.0979],\n",
      "        [0.0999],\n",
      "        [0.1006],\n",
      "        [0.1012],\n",
      "        [0.1018],\n",
      "        [0.1022],\n",
      "        [0.1025],\n",
      "        [0.1027],\n",
      "        [0.1029]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1020],\n",
      "        [0.1014],\n",
      "        [0.0974],\n",
      "        [0.0966],\n",
      "        [0.0985],\n",
      "        [0.1036],\n",
      "        [0.1005],\n",
      "        [0.1020],\n",
      "        [0.1027],\n",
      "        [0.0953]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1018],\n",
      "        [0.1074],\n",
      "        [0.1122],\n",
      "        [0.0901],\n",
      "        [0.0890],\n",
      "        [0.0912],\n",
      "        [0.1108],\n",
      "        [0.1051],\n",
      "        [0.1005],\n",
      "        [0.0920]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0897],\n",
      "        [0.1062],\n",
      "        [0.0960],\n",
      "        [0.0916],\n",
      "        [0.0930],\n",
      "        [0.1017],\n",
      "        [0.1038],\n",
      "        [0.1051],\n",
      "        [0.1061],\n",
      "        [0.1068]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0925],\n",
      "        [0.0970],\n",
      "        [0.0989],\n",
      "        [0.1001],\n",
      "        [0.1009],\n",
      "        [0.1015],\n",
      "        [0.1019],\n",
      "        [0.1022],\n",
      "        [0.1024],\n",
      "        [0.1026]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0828],\n",
      "        [0.1005],\n",
      "        [0.1047],\n",
      "        [0.0969],\n",
      "        [0.1039],\n",
      "        [0.1011],\n",
      "        [0.0991],\n",
      "        [0.1169],\n",
      "        [0.1077],\n",
      "        [0.0863]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0933],\n",
      "        [0.0931],\n",
      "        [0.0867],\n",
      "        [0.0953],\n",
      "        [0.1030],\n",
      "        [0.1117],\n",
      "        [0.1062],\n",
      "        [0.1010],\n",
      "        [0.1035],\n",
      "        [0.1062]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0842],\n",
      "        [0.0955],\n",
      "        [0.0989],\n",
      "        [0.1005],\n",
      "        [0.1017],\n",
      "        [0.1027],\n",
      "        [0.1034],\n",
      "        [0.1040],\n",
      "        [0.1044],\n",
      "        [0.1047]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0842],\n",
      "        [0.0905],\n",
      "        [0.0828],\n",
      "        [0.0922],\n",
      "        [0.1002],\n",
      "        [0.1053],\n",
      "        [0.1085],\n",
      "        [0.1107],\n",
      "        [0.1123],\n",
      "        [0.1134]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0943],\n",
      "        [0.0960],\n",
      "        [0.0923],\n",
      "        [0.0957],\n",
      "        [0.0926],\n",
      "        [0.0990],\n",
      "        [0.1085],\n",
      "        [0.0987],\n",
      "        [0.1089],\n",
      "        [0.1141]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0959],\n",
      "        [0.0953],\n",
      "        [0.0944],\n",
      "        [0.0991],\n",
      "        [0.1004],\n",
      "        [0.1015],\n",
      "        [0.1025],\n",
      "        [0.1031],\n",
      "        [0.1036],\n",
      "        [0.1040]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0914],\n",
      "        [0.0912],\n",
      "        [0.0972],\n",
      "        [0.0997],\n",
      "        [0.1014],\n",
      "        [0.1025],\n",
      "        [0.1034],\n",
      "        [0.1040],\n",
      "        [0.1044],\n",
      "        [0.1047]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1050],\n",
      "        [0.0992],\n",
      "        [0.0981],\n",
      "        [0.1031],\n",
      "        [0.1016],\n",
      "        [0.0998],\n",
      "        [0.0943],\n",
      "        [0.0972],\n",
      "        [0.1006],\n",
      "        [0.1012]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0847],\n",
      "        [0.0867],\n",
      "        [0.0961],\n",
      "        [0.0998],\n",
      "        [0.1022],\n",
      "        [0.1040],\n",
      "        [0.1054],\n",
      "        [0.1063],\n",
      "        [0.1071],\n",
      "        [0.1077]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0827],\n",
      "        [0.0815],\n",
      "        [0.0916],\n",
      "        [0.1062],\n",
      "        [0.1156],\n",
      "        [0.1191],\n",
      "        [0.1057],\n",
      "        [0.1014],\n",
      "        [0.0995],\n",
      "        [0.0968]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0855],\n",
      "        [0.0902],\n",
      "        [0.0973],\n",
      "        [0.0867],\n",
      "        [0.1042],\n",
      "        [0.0980],\n",
      "        [0.1072],\n",
      "        [0.1092],\n",
      "        [0.1104],\n",
      "        [0.1112]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1010],\n",
      "        [0.1026],\n",
      "        [0.1007],\n",
      "        [0.1042],\n",
      "        [0.0979],\n",
      "        [0.0972],\n",
      "        [0.0968],\n",
      "        [0.0983],\n",
      "        [0.0971],\n",
      "        [0.1043]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0840],\n",
      "        [0.0991],\n",
      "        [0.0907],\n",
      "        [0.0843],\n",
      "        [0.0991],\n",
      "        [0.1045],\n",
      "        [0.1074],\n",
      "        [0.1092],\n",
      "        [0.1105],\n",
      "        [0.1113]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0818],\n",
      "        [0.0827],\n",
      "        [0.0952],\n",
      "        [0.1005],\n",
      "        [0.1035],\n",
      "        [0.1054],\n",
      "        [0.1067],\n",
      "        [0.1075],\n",
      "        [0.1081],\n",
      "        [0.1085]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0959],\n",
      "        [0.1058],\n",
      "        [0.0995],\n",
      "        [0.0913],\n",
      "        [0.0927],\n",
      "        [0.0929],\n",
      "        [0.1010],\n",
      "        [0.1048],\n",
      "        [0.1072],\n",
      "        [0.1089]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0793],\n",
      "        [0.1016],\n",
      "        [0.0952],\n",
      "        [0.0942],\n",
      "        [0.0881],\n",
      "        [0.0924],\n",
      "        [0.1161],\n",
      "        [0.1152],\n",
      "        [0.1070],\n",
      "        [0.1110]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0922],\n",
      "        [0.0925],\n",
      "        [0.1007],\n",
      "        [0.1103],\n",
      "        [0.1069],\n",
      "        [0.0965],\n",
      "        [0.1086],\n",
      "        [0.1012],\n",
      "        [0.0954],\n",
      "        [0.0958]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0793],\n",
      "        [0.0827],\n",
      "        [0.0962],\n",
      "        [0.1014],\n",
      "        [0.1041],\n",
      "        [0.1057],\n",
      "        [0.1068],\n",
      "        [0.1075],\n",
      "        [0.1080],\n",
      "        [0.1083]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1061],\n",
      "        [0.0915],\n",
      "        [0.0878],\n",
      "        [0.1072],\n",
      "        [0.0924],\n",
      "        [0.0985],\n",
      "        [0.1012],\n",
      "        [0.1034],\n",
      "        [0.1052],\n",
      "        [0.1066]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0893],\n",
      "        [0.0913],\n",
      "        [0.0962],\n",
      "        [0.1066],\n",
      "        [0.1134],\n",
      "        [0.1058],\n",
      "        [0.1097],\n",
      "        [0.0976],\n",
      "        [0.0978],\n",
      "        [0.0922]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0845],\n",
      "        [0.0886],\n",
      "        [0.0869],\n",
      "        [0.0819],\n",
      "        [0.0995],\n",
      "        [0.1064],\n",
      "        [0.1102],\n",
      "        [0.1126],\n",
      "        [0.1141],\n",
      "        [0.1151]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0794],\n",
      "        [0.0795],\n",
      "        [0.0916],\n",
      "        [0.1011],\n",
      "        [0.1047],\n",
      "        [0.1068],\n",
      "        [0.1082],\n",
      "        [0.1090],\n",
      "        [0.1096],\n",
      "        [0.1101]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0914],\n",
      "        [0.0930],\n",
      "        [0.0895],\n",
      "        [0.0891],\n",
      "        [0.0950],\n",
      "        [0.1098],\n",
      "        [0.0955],\n",
      "        [0.1063],\n",
      "        [0.1132],\n",
      "        [0.1171]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0833],\n",
      "        [0.0913],\n",
      "        [0.0944],\n",
      "        [0.0955],\n",
      "        [0.1052],\n",
      "        [0.0997],\n",
      "        [0.1160],\n",
      "        [0.1049],\n",
      "        [0.0948],\n",
      "        [0.1149]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0733],\n",
      "        [0.0981],\n",
      "        [0.1010],\n",
      "        [0.1024],\n",
      "        [0.1033],\n",
      "        [0.1039],\n",
      "        [0.1042],\n",
      "        [0.1045],\n",
      "        [0.1046],\n",
      "        [0.1047]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0991],\n",
      "        [0.0822],\n",
      "        [0.1046],\n",
      "        [0.0974],\n",
      "        [0.0932],\n",
      "        [0.1022],\n",
      "        [0.0956],\n",
      "        [0.0988],\n",
      "        [0.1100],\n",
      "        [0.1168]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0986],\n",
      "        [0.0973],\n",
      "        [0.0995],\n",
      "        [0.0985],\n",
      "        [0.0889],\n",
      "        [0.0865],\n",
      "        [0.1009],\n",
      "        [0.1067],\n",
      "        [0.1103],\n",
      "        [0.1128]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0864],\n",
      "        [0.0871],\n",
      "        [0.0960],\n",
      "        [0.0936],\n",
      "        [0.0880],\n",
      "        [0.0886],\n",
      "        [0.1060],\n",
      "        [0.1143],\n",
      "        [0.1187],\n",
      "        [0.1213]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1031],\n",
      "        [0.0923],\n",
      "        [0.0943],\n",
      "        [0.0963],\n",
      "        [0.1057],\n",
      "        [0.1068],\n",
      "        [0.1028],\n",
      "        [0.1087],\n",
      "        [0.1001],\n",
      "        [0.0900]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0854],\n",
      "        [0.1004],\n",
      "        [0.0918],\n",
      "        [0.0805],\n",
      "        [0.0866],\n",
      "        [0.1033],\n",
      "        [0.1092],\n",
      "        [0.1124],\n",
      "        [0.1144],\n",
      "        [0.1158]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0750],\n",
      "        [0.0808],\n",
      "        [0.0844],\n",
      "        [0.0931],\n",
      "        [0.0956],\n",
      "        [0.1062],\n",
      "        [0.1122],\n",
      "        [0.1157],\n",
      "        [0.1178],\n",
      "        [0.1192]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0909],\n",
      "        [0.0951],\n",
      "        [0.0957],\n",
      "        [0.1009],\n",
      "        [0.1173],\n",
      "        [0.1018],\n",
      "        [0.1015],\n",
      "        [0.1084],\n",
      "        [0.1014],\n",
      "        [0.0869]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0756],\n",
      "        [0.0698],\n",
      "        [0.0924],\n",
      "        [0.1013],\n",
      "        [0.1059],\n",
      "        [0.1086],\n",
      "        [0.1102],\n",
      "        [0.1113],\n",
      "        [0.1121],\n",
      "        [0.1127]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0753],\n",
      "        [0.0804],\n",
      "        [0.0933],\n",
      "        [0.0999],\n",
      "        [0.1039],\n",
      "        [0.1066],\n",
      "        [0.1084],\n",
      "        [0.1098],\n",
      "        [0.1108],\n",
      "        [0.1116]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1083],\n",
      "        [0.1038],\n",
      "        [0.0971],\n",
      "        [0.0877],\n",
      "        [0.0909],\n",
      "        [0.0908],\n",
      "        [0.1015],\n",
      "        [0.1009],\n",
      "        [0.1123],\n",
      "        [0.1066]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0753],\n",
      "        [0.0694],\n",
      "        [0.0924],\n",
      "        [0.1014],\n",
      "        [0.1060],\n",
      "        [0.1087],\n",
      "        [0.1104],\n",
      "        [0.1115],\n",
      "        [0.1122],\n",
      "        [0.1128]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0741],\n",
      "        [0.0830],\n",
      "        [0.0937],\n",
      "        [0.0997],\n",
      "        [0.1036],\n",
      "        [0.1063],\n",
      "        [0.1082],\n",
      "        [0.1096],\n",
      "        [0.1106],\n",
      "        [0.1113]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0900],\n",
      "        [0.0987],\n",
      "        [0.0967],\n",
      "        [0.1006],\n",
      "        [0.1110],\n",
      "        [0.1122],\n",
      "        [0.1015],\n",
      "        [0.0942],\n",
      "        [0.0924],\n",
      "        [0.1028]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0879],\n",
      "        [0.0946],\n",
      "        [0.0981],\n",
      "        [0.1002],\n",
      "        [0.1016],\n",
      "        [0.1025],\n",
      "        [0.1032],\n",
      "        [0.1037],\n",
      "        [0.1040],\n",
      "        [0.1043]], grad_fn=<SoftmaxBackward>)\n",
      "epoch 1/1, loss: 0.40181756019592285\n"
     ]
    }
   ],
   "source": [
    "word_encoder = wordEncoder(VOCAB_SIZE, HIDDEN_SIZE, EMBEDDING_DIM).to(DEVICE)\n",
    "sent_encoder = sentEncoder(HIDDEN_SIZE * 2).to(DEVICE)\n",
    "model = HAN(word_encoder, sent_encoder, NUM_CLASSES, DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.NLLLoss()\n",
    "losses = []\n",
    "weights = []\n",
    "\n",
    "for i in tqdm(range(NUM_EPOCHS)):\n",
    "    current_loss = 0\n",
    "    for j in range(len(tweets[:50])):\n",
    "        tweet, score = torch.tensor(tweets[j], dtype = torch.long).to(DEVICE), torch.tensor(sent_scores[j]).to(DEVICE)\n",
    "        word_weights, sent_weights, output = model(tweet)\n",
    "        optimizer.zero_grad()\n",
    "        #current_loss += criterion(output.unsqueeze(0), score.unsqueeze(0))\n",
    "        current_loss = criterion(output.unsqueeze(0), score.unsqueeze(0))\n",
    "        current_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"epoch {i+1}/{NUM_EPOCHS}, loss: {current_loss}\")\n",
    "    losses.append(current_loss.item()/(j+1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3796, 1081, 4739, 5202, 4182, 4182, 4182, 4182, 4182, 4182],\n",
      "        [5084, 3871, 5270,  711, 5173, 3668,  777,  968, 4182, 4182],\n",
      "        [1615, 4962, 3188, 3808,  893, 2970, 4182, 4182, 4182, 4182],\n",
      "        [3244,  293, 4182, 4182, 4182, 4182, 4182, 4182, 4182, 4182]])\n",
      "Class: 3\n",
      "~~~ RESULTS ~~~\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0771],\n",
      "        [0.0996],\n",
      "        [0.0989],\n",
      "        [0.0870],\n",
      "        [0.0989],\n",
      "        [0.1040],\n",
      "        [0.1067],\n",
      "        [0.1083],\n",
      "        [0.1094],\n",
      "        [0.1101]])\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0886],\n",
      "        [0.0913],\n",
      "        [0.0833],\n",
      "        [0.0881],\n",
      "        [0.1001],\n",
      "        [0.0974],\n",
      "        [0.0992],\n",
      "        [0.1079],\n",
      "        [0.1194],\n",
      "        [0.1247]])\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0879],\n",
      "        [0.0874],\n",
      "        [0.0925],\n",
      "        [0.0901],\n",
      "        [0.0887],\n",
      "        [0.0811],\n",
      "        [0.1070],\n",
      "        [0.1172],\n",
      "        [0.1225],\n",
      "        [0.1256]])\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0721],\n",
      "        [0.0747],\n",
      "        [0.0947],\n",
      "        [0.1023],\n",
      "        [0.1061],\n",
      "        [0.1082],\n",
      "        [0.1095],\n",
      "        [0.1103],\n",
      "        [0.1108],\n",
      "        [0.1112]])\n",
      "tensor([-3.4175, -3.8605, -2.3675, -0.4045, -2.2237, -2.5651])\n",
      "[tensor([[0.0771],\n",
      "        [0.0996],\n",
      "        [0.0989],\n",
      "        [0.0870],\n",
      "        [0.0989],\n",
      "        [0.1040],\n",
      "        [0.1067],\n",
      "        [0.1083],\n",
      "        [0.1094],\n",
      "        [0.1101]]), tensor([[0.0886],\n",
      "        [0.0913],\n",
      "        [0.0833],\n",
      "        [0.0881],\n",
      "        [0.1001],\n",
      "        [0.0974],\n",
      "        [0.0992],\n",
      "        [0.1079],\n",
      "        [0.1194],\n",
      "        [0.1247]]), tensor([[0.0879],\n",
      "        [0.0874],\n",
      "        [0.0925],\n",
      "        [0.0901],\n",
      "        [0.0887],\n",
      "        [0.0811],\n",
      "        [0.1070],\n",
      "        [0.1172],\n",
      "        [0.1225],\n",
      "        [0.1256]]), tensor([[0.0721],\n",
      "        [0.0747],\n",
      "        [0.0947],\n",
      "        [0.1023],\n",
      "        [0.1061],\n",
      "        [0.1082],\n",
      "        [0.1095],\n",
      "        [0.1103],\n",
      "        [0.1108],\n",
      "        [0.1112]])]\n",
      "tensor([[0.2650],\n",
      "        [0.2340],\n",
      "        [0.2190],\n",
      "        [0.2820]])\n",
      "~~~ Prediction ~~~\n",
      "Class: 3\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    tweet, score = torch.tensor(tweets[50], dtype = torch.long).to(DEVICE), torch.tensor(sent_scores[j]).to(DEVICE)\n",
    "    print(tweet)\n",
    "    print(\"Class:\", score.item())\n",
    "\n",
    "    print(\"~~~ RESULTS ~~~\")\n",
    "    word_weights, sent_weights, output = model(tweet)\n",
    "    print(output)\n",
    "    print(word_weights)\n",
    "    print(sent_weights)\n",
    "    print(\"~~~ Prediction ~~~\")\n",
    "    _, idx = torch.max(output, 0)\n",
    "    print(\"Class:\",idx.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing a rover nasaames <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "it uses same tech as self driving cars <pad> <pad>\n",
      "stateofnasa nasasocial nasa û_ http t <pad> <pad> <pad> <pad>\n",
      "co pin2j8fusj <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Correct: 3\n"
     ]
    }
   ],
   "source": [
    "for t in tweet:\n",
    "    t = t.numpy()\n",
    "    sent = \" \".join([unique_tokens[w] for w in t])\n",
    "    print(sent)\n",
    "\n",
    "if score.item() == idx.item():\n",
    "    print(f\"Correct: {score.item()}\")\n",
    "else:\n",
    "    print(f\"Truth: {score.item()}, Predicted:{idx.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}