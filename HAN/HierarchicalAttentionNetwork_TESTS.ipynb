{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nhttps://buomsoo-kim.github.io/attention/2020/03/26/Attention-mechanism-16.md/\\n\\nsentiment scores:\\nvery positive = 5\\nslightly positive = 4\\nneutral = 3\\nslightly negative = 2\\nvery negative = 1\\n\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://buomsoo-kim.github.io/attention/2020/03/26/Attention-mechanism-16.md/\n",
    "\n",
    "sentiment scores:\n",
    "very positive = 5\n",
    "slightly positive = 4\n",
    "neutral = 3\n",
    "slightly negative = 2\n",
    "very negative = 1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
      "0  724227031     True      golden                 236               NaN   \n",
      "1  724227032     True      golden                 231               NaN   \n",
      "2  724227033     True      golden                 233               NaN   \n",
      "3  724227034     True      golden                 240               NaN   \n",
      "4  724227035     True      golden                 240               NaN   \n",
      "\n",
      "  sentiment  sentiment:confidence  our_id sentiment_gold  \\\n",
      "0         5                0.7579   10001           5\\n4   \n",
      "1         5                0.8775   10002           5\\n4   \n",
      "2         2                0.6805   10003           2\\n1   \n",
      "3         2                0.8820   10004           2\\n1   \n",
      "4         3                1.0000   10005              3   \n",
      "\n",
      "                               sentiment_gold_reason  \\\n",
      "0  Author is excited about the development of the...   \n",
      "1  Author is excited that driverless cars will be...   \n",
      "2  The author is skeptical of the safety and reli...   \n",
      "3    The author is skeptical of the project's value.   \n",
      "4  Author is making an observation without expres...   \n",
      "\n",
      "                                                text  \n",
      "0  Two places I'd invest all my money if I could:...  \n",
      "1  Awesome! Google driverless cars will help the ...  \n",
      "2  If Google maps can't keep up with road constru...  \n",
      "3  Autonomous cars seem way overhyped given the t...  \n",
      "4  Just saw Google self-driving car on I-34. It w...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Twitter-sentiment-self-drive-DFE.csv\", encoding = 'latin-1')\n",
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 5604.88it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_INSTANCES = 3000\n",
    "MAX_SENT_LEN = 10\n",
    "tweets = []\n",
    "sent_scores = []\n",
    "unique_tokens = set()\n",
    "\n",
    "for i in tqdm(range(NUM_INSTANCES)):\n",
    "    rand_idx = np.random.randint(len(data))\n",
    "\n",
    "    tweet = []\n",
    "    sentences = data['text'].iloc[rand_idx].split(\".\")\n",
    "    for sent in sentences:\n",
    "        if len(sent) != 0:\n",
    "            # Get only words\n",
    "            sent = [x.lower() for x in re.findall(r\"\\w+\", sent)]\n",
    "            if len(sent) >= MAX_SENT_LEN:\n",
    "                sent = sent[:MAX_SENT_LEN]\n",
    "            else:\n",
    "                for _ in range(MAX_SENT_LEN - len(sent)):\n",
    "                    sent.append(\"<pad>\")\n",
    "\n",
    "            tweet.append(sent)\n",
    "            unique_tokens.update(sent)\n",
    "    tweets.append(tweet)\n",
    "    if data['sentiment'].iloc[rand_idx] == \"not_relevant\":\n",
    "        sent_scores.append(0)\n",
    "    else:\n",
    "        sent_scores.append(int(data[\"sentiment\"].iloc[rand_idx]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Text from tweet 14\n",
      "[['everyone', 'will', 'have', 'self', 'driving', 'cars', 'by', '2026', 'analyst', 'says'], ['co', 'vzu7fmlb5k', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "Number of sentences in tweet\n",
      "2\n",
      "Sentiment score for the tweet\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets))\n",
    "tweet_id = 14\n",
    "print(f\"Text from tweet {tweet_id}\")\n",
    "print(tweets[tweet_id])\n",
    "print(\"Number of sentences in tweet\")\n",
    "print(len(tweets[tweet_id]))\n",
    "print(\"Sentiment score for the tweet\")\n",
    "print(sent_scores[tweet_id])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unique tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens\n",
      "6330\n",
      "Example of the first 20 tokens\n",
      "['music', 'ralphpaglia', 'cyberdyne', 'otcccc', 'laws', 'd8g9obxryq', 'lookin', 'robo', 'newyorker', 'effect', 'scottkirsner', 'bbnqzhcvc2ì', 'hi', 'jpfpces3us', 'tv', 'story', 'yes', 'reply', 'zuhwrs7j', 'sets']\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = list(unique_tokens)\n",
    "print(\"Number of unique tokens\")\n",
    "print(len(unique_tokens))\n",
    "print(\"Example of the first 20 tokens\")\n",
    "print(unique_tokens[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Numericalize each token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:09<00:00, 321.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# encode each token into index\n",
    "for i in tqdm(range(len(tweets))):\n",
    "#for i in range(len(tweets)):\n",
    "    for j in range(len(tweets[i])):\n",
    "        tweets[i][j] = [unique_tokens.index(x) for x in tweets[i][j]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 14 in numbers\n",
      "[[5617, 1271, 4636, 3121, 5990, 2488, 4515, 4774, 4294, 276], [2718, 5160, 6184, 6184, 6184, 6184, 6184, 6184, 6184, 6184]]\n",
      "Number of sentences in tweet\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tweet {tweet_id} in numbers\")\n",
    "print(tweets[tweet_id])\n",
    "print(\"Number of sentences in tweet\")\n",
    "print(len(tweets[tweet_id]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 6\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(unique_tokens)\n",
    "NUM_CLASSES = len(set(sent_scores))\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 1#0\n",
    "HIDDEN_SIZE = 16\n",
    "EMBEDDING_DIM = 30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Classes\", NUM_CLASSES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class wordEncoder(nn.Module):\n",
    "  def __init__(self, vocab_size, hidden_size, embedding_dim):\n",
    "    super(wordEncoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.vocab_size = vocab_size\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = nn.GRU(embedding_dim, hidden_size, bidirectional = True)\n",
    "\n",
    "  def forward(self, word, h0):\n",
    "    word = self.embedding(word).unsqueeze(0).unsqueeze(1)\n",
    "    out, h0 = self.gru(word, h0)\n",
    "    return out, h0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class sentEncoder(nn.Module):\n",
    "  def __init__(self, hidden_size):\n",
    "    super(sentEncoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, bidirectional = True)\n",
    "\n",
    "  def forward(self, sentence, h0):\n",
    "    sentence = sentence.unsqueeze(0).unsqueeze(1)\n",
    "    out, h0 = self.gru(sentence)\n",
    "    return out, h0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hierarchical Attention Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "  def __init__(self, wordEncoder, sentEncoder, num_classes, device):\n",
    "    super(HAN, self).__init__()\n",
    "    self.wordEncoder = wordEncoder\n",
    "    self.sentEncoder = sentEncoder\n",
    "    self.device = device\n",
    "    #self.softmax = nn.Softmax(dim=1)\n",
    "    self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    # word-level attention\n",
    "    self.word_attention = nn.Linear(self.wordEncoder.hidden_size*2, self.wordEncoder.hidden_size*2)\n",
    "    self.u_w = nn.Linear(self.wordEncoder.hidden_size*2, 1, bias = False)\n",
    "\n",
    "    # sentence-level attention\n",
    "    self.sent_attention = nn.Linear(self.sentEncoder.hidden_size * 2, self.sentEncoder.hidden_size*2)\n",
    "    self.u_s = nn.Linear(self.sentEncoder.hidden_size*2, 1, bias = False)\n",
    "\n",
    "    # final layer\n",
    "    self.dense_out = nn.Linear(self.sentEncoder.hidden_size*2, num_classes)\n",
    "    self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "\n",
    "  def forward(self, document):\n",
    "    word_attention_weights = []\n",
    "    sentenc_out = torch.zeros((document.size(0), 2, self.sentEncoder.hidden_size)).to(self.device)\n",
    "    # iterate on sentences\n",
    "    h0_sent = torch.zeros(2, 1, self.sentEncoder.hidden_size, dtype = torch.float).to(self.device)\n",
    "    for i in range(document.size(0)):\n",
    "      sent = document[i]\n",
    "      wordenc_out = torch.zeros((sent.size(0), 2, self.wordEncoder.hidden_size)).to(self.device)\n",
    "      h0_word = torch.zeros(2, 1, self.wordEncoder.hidden_size, dtype = torch.float).to(self.device)\n",
    "      # iterate on words\n",
    "      for j in range(sent.size(0)):\n",
    "        _, h0_word = self.wordEncoder(sent[j], h0_word)\n",
    "        wordenc_out[j] = h0_word.squeeze()\n",
    "      #print(wordenc_out)\n",
    "      wordenc_out = wordenc_out.view(wordenc_out.size(0), -1)\n",
    "      u_word = torch.tanh(self.word_attention(wordenc_out))\n",
    "      #print()\n",
    "      #print(\"u_word\")\n",
    "      #print(u_word)\n",
    "\n",
    "      x = self.u_w(u_word)\n",
    "      #print(\"~~~~~ x ~~~~~\")\n",
    "      #print(x)\n",
    "      #print(x.shape)\n",
    "      #word_weights = self.softmax(self.u_w(u_word))\n",
    "      #aligned_weights_ = F.softmax(aligned_weights.unsqueeze(0))\n",
    "      word_weights = self.softmax(x)\n",
    "\n",
    "      #print()\n",
    "      #print(\"word_weights\")\n",
    "      #print(word_weights)\n",
    "      word_attention_weights.append(word_weights)\n",
    "\n",
    "      sent_summ_vector = (u_word * word_weights).sum(axis=0)\n",
    "\n",
    "      _, h0_sent = self.sentEncoder(sent_summ_vector, h0_sent)\n",
    "      sentenc_out[i] = h0_sent.squeeze()\n",
    "    sentenc_out = sentenc_out.view(sentenc_out.size(0), -1)\n",
    "    u_sent = torch.tanh(self.sent_attention(sentenc_out))\n",
    "    sent_weights = self.softmax(self.u_s(u_sent))\n",
    "    doc_summ_vector = (u_sent * sent_weights).sum(axis=0)\n",
    "    out = self.dense_out(doc_summ_vector)\n",
    "    return word_attention_weights, sent_weights, self.log_softmax(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word_weights\n",
      "tensor([[0.0861],\n",
      "        [0.0833],\n",
      "        [0.1047],\n",
      "        [0.1055],\n",
      "        [0.1011],\n",
      "        [0.1135],\n",
      "        [0.1072],\n",
      "        [0.1005],\n",
      "        [0.0989],\n",
      "        [0.0993]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1048],\n",
      "        [0.1181],\n",
      "        [0.1057],\n",
      "        [0.1000],\n",
      "        [0.0972],\n",
      "        [0.0958],\n",
      "        [0.0951],\n",
      "        [0.0946],\n",
      "        [0.0944],\n",
      "        [0.0943]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0840],\n",
      "        [0.1063],\n",
      "        [0.0999],\n",
      "        [0.0965],\n",
      "        [0.1111],\n",
      "        [0.1059],\n",
      "        [0.1007],\n",
      "        [0.1086],\n",
      "        [0.1059],\n",
      "        [0.0812]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1049],\n",
      "        [0.1004],\n",
      "        [0.0984],\n",
      "        [0.0981],\n",
      "        [0.0985],\n",
      "        [0.0991],\n",
      "        [0.0996],\n",
      "        [0.1000],\n",
      "        [0.1004],\n",
      "        [0.1006]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0985],\n",
      "        [0.0812],\n",
      "        [0.0785],\n",
      "        [0.0997],\n",
      "        [0.1110],\n",
      "        [0.1207],\n",
      "        [0.1077],\n",
      "        [0.1094],\n",
      "        [0.0988],\n",
      "        [0.0945]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1011],\n",
      "        [0.0987],\n",
      "        [0.0938],\n",
      "        [0.0956],\n",
      "        [0.0978],\n",
      "        [0.0989],\n",
      "        [0.0953],\n",
      "        [0.1077],\n",
      "        [0.1039],\n",
      "        [0.1071]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0980],\n",
      "        [0.0830],\n",
      "        [0.0813],\n",
      "        [0.0939],\n",
      "        [0.1152],\n",
      "        [0.1031],\n",
      "        [0.1116],\n",
      "        [0.1105],\n",
      "        [0.1065],\n",
      "        [0.0969]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1054],\n",
      "        [0.0976],\n",
      "        [0.1159],\n",
      "        [0.0981],\n",
      "        [0.0924],\n",
      "        [0.1050],\n",
      "        [0.0986],\n",
      "        [0.0963],\n",
      "        [0.0954],\n",
      "        [0.0952]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1056],\n",
      "        [0.1058],\n",
      "        [0.1013],\n",
      "        [0.0992],\n",
      "        [0.0982],\n",
      "        [0.0978],\n",
      "        [0.0978],\n",
      "        [0.0979],\n",
      "        [0.0981],\n",
      "        [0.0983]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1050],\n",
      "        [0.1053],\n",
      "        [0.1010],\n",
      "        [0.0992],\n",
      "        [0.0984],\n",
      "        [0.0982],\n",
      "        [0.0981],\n",
      "        [0.0982],\n",
      "        [0.0983],\n",
      "        [0.0984]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1042],\n",
      "        [0.1013],\n",
      "        [0.0955],\n",
      "        [0.0998],\n",
      "        [0.0876],\n",
      "        [0.1079],\n",
      "        [0.1041],\n",
      "        [0.1008],\n",
      "        [0.0967],\n",
      "        [0.1021]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0992],\n",
      "        [0.1028],\n",
      "        [0.0953],\n",
      "        [0.0930],\n",
      "        [0.0958],\n",
      "        [0.1151],\n",
      "        [0.1112],\n",
      "        [0.1002],\n",
      "        [0.0964],\n",
      "        [0.0910]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1054],\n",
      "        [0.1106],\n",
      "        [0.1012],\n",
      "        [0.0982],\n",
      "        [0.0972],\n",
      "        [0.0970],\n",
      "        [0.0972],\n",
      "        [0.0975],\n",
      "        [0.0978],\n",
      "        [0.0980]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0804],\n",
      "        [0.0779],\n",
      "        [0.0967],\n",
      "        [0.0941],\n",
      "        [0.1133],\n",
      "        [0.1207],\n",
      "        [0.1152],\n",
      "        [0.1059],\n",
      "        [0.0964],\n",
      "        [0.0994]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1032],\n",
      "        [0.1131],\n",
      "        [0.0990],\n",
      "        [0.1157],\n",
      "        [0.0932],\n",
      "        [0.0872],\n",
      "        [0.0985],\n",
      "        [0.1053],\n",
      "        [0.0924],\n",
      "        [0.0924]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0988],\n",
      "        [0.0889],\n",
      "        [0.0854],\n",
      "        [0.1067],\n",
      "        [0.1143],\n",
      "        [0.1136],\n",
      "        [0.1052],\n",
      "        [0.1016],\n",
      "        [0.0936],\n",
      "        [0.0919]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1054],\n",
      "        [0.1012],\n",
      "        [0.0985],\n",
      "        [0.0983],\n",
      "        [0.0986],\n",
      "        [0.0990],\n",
      "        [0.0994],\n",
      "        [0.0997],\n",
      "        [0.0999],\n",
      "        [0.1001]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0980],\n",
      "        [0.0986],\n",
      "        [0.1042],\n",
      "        [0.0994],\n",
      "        [0.0969],\n",
      "        [0.1051],\n",
      "        [0.1132],\n",
      "        [0.1086],\n",
      "        [0.0900],\n",
      "        [0.0861]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1165],\n",
      "        [0.1095],\n",
      "        [0.1007],\n",
      "        [0.0977],\n",
      "        [0.0962],\n",
      "        [0.0957],\n",
      "        [0.0957],\n",
      "        [0.0958],\n",
      "        [0.0960],\n",
      "        [0.0962]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1019],\n",
      "        [0.1097],\n",
      "        [0.1069],\n",
      "        [0.0888],\n",
      "        [0.0967],\n",
      "        [0.1004],\n",
      "        [0.1056],\n",
      "        [0.0981],\n",
      "        [0.0963],\n",
      "        [0.0954]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1002],\n",
      "        [0.1049],\n",
      "        [0.1067],\n",
      "        [0.1219],\n",
      "        [0.0950],\n",
      "        [0.0871],\n",
      "        [0.0971],\n",
      "        [0.0921],\n",
      "        [0.0980],\n",
      "        [0.0971]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1060],\n",
      "        [0.1099],\n",
      "        [0.0970],\n",
      "        [0.1081],\n",
      "        [0.0960],\n",
      "        [0.0974],\n",
      "        [0.0930],\n",
      "        [0.1040],\n",
      "        [0.0960],\n",
      "        [0.0927]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1060],\n",
      "        [0.1136],\n",
      "        [0.0983],\n",
      "        [0.1005],\n",
      "        [0.1052],\n",
      "        [0.0994],\n",
      "        [0.0976],\n",
      "        [0.0943],\n",
      "        [0.0929],\n",
      "        [0.0923]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0964],\n",
      "        [0.1201],\n",
      "        [0.1084],\n",
      "        [0.1019],\n",
      "        [0.0984],\n",
      "        [0.0965],\n",
      "        [0.0954],\n",
      "        [0.0947],\n",
      "        [0.0942],\n",
      "        [0.0939]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1033],\n",
      "        [0.0953],\n",
      "        [0.1167],\n",
      "        [0.1016],\n",
      "        [0.1102],\n",
      "        [0.1083],\n",
      "        [0.0902],\n",
      "        [0.0862],\n",
      "        [0.0959],\n",
      "        [0.0922]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1030],\n",
      "        [0.1065],\n",
      "        [0.1011],\n",
      "        [0.0990],\n",
      "        [0.0983],\n",
      "        [0.0981],\n",
      "        [0.0982],\n",
      "        [0.0984],\n",
      "        [0.0986],\n",
      "        [0.0988]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1044],\n",
      "        [0.0962],\n",
      "        [0.1027],\n",
      "        [0.1017],\n",
      "        [0.0948],\n",
      "        [0.0995],\n",
      "        [0.0919],\n",
      "        [0.1141],\n",
      "        [0.0985],\n",
      "        [0.0963]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0947],\n",
      "        [0.0878],\n",
      "        [0.0902],\n",
      "        [0.1050],\n",
      "        [0.1143],\n",
      "        [0.1118],\n",
      "        [0.1115],\n",
      "        [0.1040],\n",
      "        [0.0927],\n",
      "        [0.0880]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1065],\n",
      "        [0.1065],\n",
      "        [0.0964],\n",
      "        [0.1026],\n",
      "        [0.0902],\n",
      "        [0.0873],\n",
      "        [0.0968],\n",
      "        [0.1036],\n",
      "        [0.1013],\n",
      "        [0.1089]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1147],\n",
      "        [0.1052],\n",
      "        [0.1026],\n",
      "        [0.0991],\n",
      "        [0.0972],\n",
      "        [0.0963],\n",
      "        [0.0961],\n",
      "        [0.0961],\n",
      "        [0.0963],\n",
      "        [0.0965]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1021],\n",
      "        [0.1053],\n",
      "        [0.1012],\n",
      "        [0.0997],\n",
      "        [0.0989],\n",
      "        [0.0986],\n",
      "        [0.0985],\n",
      "        [0.0985],\n",
      "        [0.0986],\n",
      "        [0.0986]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0927],\n",
      "        [0.1093],\n",
      "        [0.1066],\n",
      "        [0.1063],\n",
      "        [0.0886],\n",
      "        [0.0915],\n",
      "        [0.1060],\n",
      "        [0.1088],\n",
      "        [0.0965],\n",
      "        [0.0938]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0974],\n",
      "        [0.0977],\n",
      "        [0.1082],\n",
      "        [0.1165],\n",
      "        [0.1034],\n",
      "        [0.0981],\n",
      "        [0.0957],\n",
      "        [0.0947],\n",
      "        [0.0942],\n",
      "        [0.0941]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1008],\n",
      "        [0.0911],\n",
      "        [0.1036],\n",
      "        [0.0982],\n",
      "        [0.1098],\n",
      "        [0.1070],\n",
      "        [0.0935],\n",
      "        [0.1036],\n",
      "        [0.0951],\n",
      "        [0.0973]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1060],\n",
      "        [0.1023],\n",
      "        [0.0993],\n",
      "        [0.0984],\n",
      "        [0.0983],\n",
      "        [0.0986],\n",
      "        [0.0989],\n",
      "        [0.0991],\n",
      "        [0.0994],\n",
      "        [0.0996]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1070],\n",
      "        [0.1007],\n",
      "        [0.1052],\n",
      "        [0.0888],\n",
      "        [0.0843],\n",
      "        [0.1054],\n",
      "        [0.1040],\n",
      "        [0.1025],\n",
      "        [0.1014],\n",
      "        [0.1007]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0898],\n",
      "        [0.1143],\n",
      "        [0.1012],\n",
      "        [0.1013],\n",
      "        [0.0923],\n",
      "        [0.0964],\n",
      "        [0.0937],\n",
      "        [0.1026],\n",
      "        [0.1034],\n",
      "        [0.1050]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1060],\n",
      "        [0.0988],\n",
      "        [0.1181],\n",
      "        [0.1028],\n",
      "        [0.1035],\n",
      "        [0.0967],\n",
      "        [0.0944],\n",
      "        [0.0935],\n",
      "        [0.0932],\n",
      "        [0.0931]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0903],\n",
      "        [0.0963],\n",
      "        [0.1102],\n",
      "        [0.0991],\n",
      "        [0.1029],\n",
      "        [0.0958],\n",
      "        [0.0946],\n",
      "        [0.1056],\n",
      "        [0.1015],\n",
      "        [0.1038]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0976],\n",
      "        [0.0819],\n",
      "        [0.0974],\n",
      "        [0.1004],\n",
      "        [0.0967],\n",
      "        [0.1056],\n",
      "        [0.1190],\n",
      "        [0.0998],\n",
      "        [0.1058],\n",
      "        [0.0959]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0971],\n",
      "        [0.0957],\n",
      "        [0.1064],\n",
      "        [0.1110],\n",
      "        [0.1060],\n",
      "        [0.1038],\n",
      "        [0.1058],\n",
      "        [0.0953],\n",
      "        [0.0875],\n",
      "        [0.0913]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1097],\n",
      "        [0.1019],\n",
      "        [0.0990],\n",
      "        [0.0981],\n",
      "        [0.0980],\n",
      "        [0.0981],\n",
      "        [0.0984],\n",
      "        [0.0987],\n",
      "        [0.0989],\n",
      "        [0.0991]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1068],\n",
      "        [0.1025],\n",
      "        [0.0943],\n",
      "        [0.0912],\n",
      "        [0.0989],\n",
      "        [0.0995],\n",
      "        [0.1093],\n",
      "        [0.1012],\n",
      "        [0.0988],\n",
      "        [0.0975]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1006],\n",
      "        [0.1006],\n",
      "        [0.1000],\n",
      "        [0.0999],\n",
      "        [0.0998],\n",
      "        [0.0998],\n",
      "        [0.0998],\n",
      "        [0.0998],\n",
      "        [0.0998],\n",
      "        [0.0999]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1125],\n",
      "        [0.0985],\n",
      "        [0.0958],\n",
      "        [0.1035],\n",
      "        [0.0983],\n",
      "        [0.0947],\n",
      "        [0.0960],\n",
      "        [0.0984],\n",
      "        [0.1005],\n",
      "        [0.1018]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0966],\n",
      "        [0.0981],\n",
      "        [0.0963],\n",
      "        [0.1030],\n",
      "        [0.0957],\n",
      "        [0.0924],\n",
      "        [0.0963],\n",
      "        [0.1075],\n",
      "        [0.1170],\n",
      "        [0.0972]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0892],\n",
      "        [0.0979],\n",
      "        [0.0974],\n",
      "        [0.1086],\n",
      "        [0.0962],\n",
      "        [0.0929],\n",
      "        [0.1058],\n",
      "        [0.1088],\n",
      "        [0.1030],\n",
      "        [0.1003]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1083],\n",
      "        [0.0936],\n",
      "        [0.0883],\n",
      "        [0.0798],\n",
      "        [0.0815],\n",
      "        [0.0998],\n",
      "        [0.1048],\n",
      "        [0.1259],\n",
      "        [0.1122],\n",
      "        [0.1059]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0970],\n",
      "        [0.0943],\n",
      "        [0.0899],\n",
      "        [0.1198],\n",
      "        [0.1021],\n",
      "        [0.0953],\n",
      "        [0.1018],\n",
      "        [0.1005],\n",
      "        [0.0997],\n",
      "        [0.0997]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1003],\n",
      "        [0.1024],\n",
      "        [0.0996],\n",
      "        [0.0973],\n",
      "        [0.0981],\n",
      "        [0.0988],\n",
      "        [0.0997],\n",
      "        [0.1006],\n",
      "        [0.1013],\n",
      "        [0.1019]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0899],\n",
      "        [0.1104],\n",
      "        [0.0974],\n",
      "        [0.1069],\n",
      "        [0.1171],\n",
      "        [0.1049],\n",
      "        [0.0964],\n",
      "        [0.0932],\n",
      "        [0.0921],\n",
      "        [0.0917]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1121],\n",
      "        [0.1002],\n",
      "        [0.1084],\n",
      "        [0.0986],\n",
      "        [0.1070],\n",
      "        [0.0976],\n",
      "        [0.0948],\n",
      "        [0.0938],\n",
      "        [0.0937],\n",
      "        [0.0938]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0914],\n",
      "        [0.1074],\n",
      "        [0.1123],\n",
      "        [0.1043],\n",
      "        [0.0987],\n",
      "        [0.1071],\n",
      "        [0.0987],\n",
      "        [0.0971],\n",
      "        [0.0924],\n",
      "        [0.0906]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0978],\n",
      "        [0.1057],\n",
      "        [0.1054],\n",
      "        [0.1144],\n",
      "        [0.0973],\n",
      "        [0.0980],\n",
      "        [0.0980],\n",
      "        [0.0975],\n",
      "        [0.0963],\n",
      "        [0.0896]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0896],\n",
      "        [0.0972],\n",
      "        [0.0985],\n",
      "        [0.0964],\n",
      "        [0.1040],\n",
      "        [0.1057],\n",
      "        [0.1038],\n",
      "        [0.1054],\n",
      "        [0.1099],\n",
      "        [0.0895]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0942],\n",
      "        [0.1127],\n",
      "        [0.1037],\n",
      "        [0.0990],\n",
      "        [0.0988],\n",
      "        [0.0981],\n",
      "        [0.0980],\n",
      "        [0.0982],\n",
      "        [0.0985],\n",
      "        [0.0988]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0979],\n",
      "        [0.1091],\n",
      "        [0.1020],\n",
      "        [0.0990],\n",
      "        [0.0981],\n",
      "        [0.0981],\n",
      "        [0.0984],\n",
      "        [0.0988],\n",
      "        [0.0991],\n",
      "        [0.0995]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0850],\n",
      "        [0.0911],\n",
      "        [0.1057],\n",
      "        [0.1029],\n",
      "        [0.1117],\n",
      "        [0.1063],\n",
      "        [0.1020],\n",
      "        [0.0996],\n",
      "        [0.0983],\n",
      "        [0.0975]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0886],\n",
      "        [0.0913],\n",
      "        [0.0945],\n",
      "        [0.1001],\n",
      "        [0.1108],\n",
      "        [0.1026],\n",
      "        [0.0935],\n",
      "        [0.0997],\n",
      "        [0.1039],\n",
      "        [0.1149]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0975],\n",
      "        [0.0956],\n",
      "        [0.0989],\n",
      "        [0.0995],\n",
      "        [0.1001],\n",
      "        [0.1007],\n",
      "        [0.1013],\n",
      "        [0.1018],\n",
      "        [0.1022],\n",
      "        [0.1025]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0979],\n",
      "        [0.0995],\n",
      "        [0.0997],\n",
      "        [0.0995],\n",
      "        [0.0997],\n",
      "        [0.1001],\n",
      "        [0.1004],\n",
      "        [0.1008],\n",
      "        [0.1011],\n",
      "        [0.1013]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0923],\n",
      "        [0.1015],\n",
      "        [0.0933],\n",
      "        [0.0953],\n",
      "        [0.0986],\n",
      "        [0.1088],\n",
      "        [0.0954],\n",
      "        [0.0925],\n",
      "        [0.1093],\n",
      "        [0.1129]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0913],\n",
      "        [0.0995],\n",
      "        [0.1032],\n",
      "        [0.0980],\n",
      "        [0.0962],\n",
      "        [0.1002],\n",
      "        [0.1015],\n",
      "        [0.1025],\n",
      "        [0.1034],\n",
      "        [0.1041]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0938],\n",
      "        [0.1082],\n",
      "        [0.1016],\n",
      "        [0.0994],\n",
      "        [0.0989],\n",
      "        [0.0991],\n",
      "        [0.0994],\n",
      "        [0.0997],\n",
      "        [0.0999],\n",
      "        [0.1001]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1135],\n",
      "        [0.1057],\n",
      "        [0.1019],\n",
      "        [0.0922],\n",
      "        [0.1038],\n",
      "        [0.0993],\n",
      "        [0.0857],\n",
      "        [0.0988],\n",
      "        [0.1083],\n",
      "        [0.0909]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1020],\n",
      "        [0.1020],\n",
      "        [0.0870],\n",
      "        [0.0945],\n",
      "        [0.0979],\n",
      "        [0.1003],\n",
      "        [0.1022],\n",
      "        [0.1036],\n",
      "        [0.1048],\n",
      "        [0.1056]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0858],\n",
      "        [0.0914],\n",
      "        [0.0938],\n",
      "        [0.1076],\n",
      "        [0.1050],\n",
      "        [0.1024],\n",
      "        [0.1032],\n",
      "        [0.0982],\n",
      "        [0.1008],\n",
      "        [0.1118]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0997],\n",
      "        [0.1030],\n",
      "        [0.1093],\n",
      "        [0.1011],\n",
      "        [0.0961],\n",
      "        [0.0906],\n",
      "        [0.0943],\n",
      "        [0.0941],\n",
      "        [0.1075],\n",
      "        [0.1044]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0922],\n",
      "        [0.0890],\n",
      "        [0.0970],\n",
      "        [0.0997],\n",
      "        [0.1014],\n",
      "        [0.1027],\n",
      "        [0.1036],\n",
      "        [0.1043],\n",
      "        [0.1048],\n",
      "        [0.1052]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0852],\n",
      "        [0.1016],\n",
      "        [0.1111],\n",
      "        [0.1055],\n",
      "        [0.1017],\n",
      "        [0.0999],\n",
      "        [0.0992],\n",
      "        [0.0988],\n",
      "        [0.0986],\n",
      "        [0.0984]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0984],\n",
      "        [0.1057],\n",
      "        [0.1130],\n",
      "        [0.1084],\n",
      "        [0.0914],\n",
      "        [0.0966],\n",
      "        [0.0924],\n",
      "        [0.0827],\n",
      "        [0.1022],\n",
      "        [0.1093]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0909],\n",
      "        [0.0874],\n",
      "        [0.0965],\n",
      "        [0.0998],\n",
      "        [0.1018],\n",
      "        [0.1032],\n",
      "        [0.1042],\n",
      "        [0.1049],\n",
      "        [0.1055],\n",
      "        [0.1059]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0914],\n",
      "        [0.1043],\n",
      "        [0.0961],\n",
      "        [0.0919],\n",
      "        [0.0990],\n",
      "        [0.1013],\n",
      "        [0.1027],\n",
      "        [0.1037],\n",
      "        [0.1045],\n",
      "        [0.1050]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0883],\n",
      "        [0.0979],\n",
      "        [0.0999],\n",
      "        [0.1006],\n",
      "        [0.1012],\n",
      "        [0.1018],\n",
      "        [0.1022],\n",
      "        [0.1025],\n",
      "        [0.1027],\n",
      "        [0.1029]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1020],\n",
      "        [0.1014],\n",
      "        [0.0974],\n",
      "        [0.0966],\n",
      "        [0.0985],\n",
      "        [0.1036],\n",
      "        [0.1005],\n",
      "        [0.1020],\n",
      "        [0.1027],\n",
      "        [0.0953]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1018],\n",
      "        [0.1074],\n",
      "        [0.1122],\n",
      "        [0.0901],\n",
      "        [0.0890],\n",
      "        [0.0912],\n",
      "        [0.1108],\n",
      "        [0.1051],\n",
      "        [0.1005],\n",
      "        [0.0920]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0897],\n",
      "        [0.1062],\n",
      "        [0.0960],\n",
      "        [0.0916],\n",
      "        [0.0930],\n",
      "        [0.1017],\n",
      "        [0.1038],\n",
      "        [0.1051],\n",
      "        [0.1061],\n",
      "        [0.1068]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0925],\n",
      "        [0.0970],\n",
      "        [0.0989],\n",
      "        [0.1001],\n",
      "        [0.1009],\n",
      "        [0.1015],\n",
      "        [0.1019],\n",
      "        [0.1022],\n",
      "        [0.1024],\n",
      "        [0.1026]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0828],\n",
      "        [0.1005],\n",
      "        [0.1047],\n",
      "        [0.0969],\n",
      "        [0.1039],\n",
      "        [0.1011],\n",
      "        [0.0991],\n",
      "        [0.1169],\n",
      "        [0.1077],\n",
      "        [0.0863]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0933],\n",
      "        [0.0931],\n",
      "        [0.0867],\n",
      "        [0.0953],\n",
      "        [0.1030],\n",
      "        [0.1117],\n",
      "        [0.1062],\n",
      "        [0.1010],\n",
      "        [0.1035],\n",
      "        [0.1062]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0842],\n",
      "        [0.0955],\n",
      "        [0.0989],\n",
      "        [0.1005],\n",
      "        [0.1017],\n",
      "        [0.1027],\n",
      "        [0.1034],\n",
      "        [0.1040],\n",
      "        [0.1044],\n",
      "        [0.1047]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0842],\n",
      "        [0.0905],\n",
      "        [0.0828],\n",
      "        [0.0922],\n",
      "        [0.1002],\n",
      "        [0.1053],\n",
      "        [0.1085],\n",
      "        [0.1107],\n",
      "        [0.1123],\n",
      "        [0.1134]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0943],\n",
      "        [0.0960],\n",
      "        [0.0923],\n",
      "        [0.0957],\n",
      "        [0.0926],\n",
      "        [0.0990],\n",
      "        [0.1085],\n",
      "        [0.0987],\n",
      "        [0.1089],\n",
      "        [0.1141]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0959],\n",
      "        [0.0953],\n",
      "        [0.0944],\n",
      "        [0.0991],\n",
      "        [0.1004],\n",
      "        [0.1015],\n",
      "        [0.1025],\n",
      "        [0.1031],\n",
      "        [0.1036],\n",
      "        [0.1040]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0914],\n",
      "        [0.0912],\n",
      "        [0.0972],\n",
      "        [0.0997],\n",
      "        [0.1014],\n",
      "        [0.1025],\n",
      "        [0.1034],\n",
      "        [0.1040],\n",
      "        [0.1044],\n",
      "        [0.1047]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1050],\n",
      "        [0.0992],\n",
      "        [0.0981],\n",
      "        [0.1031],\n",
      "        [0.1016],\n",
      "        [0.0998],\n",
      "        [0.0943],\n",
      "        [0.0972],\n",
      "        [0.1006],\n",
      "        [0.1012]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0847],\n",
      "        [0.0867],\n",
      "        [0.0961],\n",
      "        [0.0998],\n",
      "        [0.1022],\n",
      "        [0.1040],\n",
      "        [0.1054],\n",
      "        [0.1063],\n",
      "        [0.1071],\n",
      "        [0.1077]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0827],\n",
      "        [0.0815],\n",
      "        [0.0916],\n",
      "        [0.1062],\n",
      "        [0.1156],\n",
      "        [0.1191],\n",
      "        [0.1057],\n",
      "        [0.1014],\n",
      "        [0.0995],\n",
      "        [0.0968]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0855],\n",
      "        [0.0902],\n",
      "        [0.0973],\n",
      "        [0.0867],\n",
      "        [0.1042],\n",
      "        [0.0980],\n",
      "        [0.1072],\n",
      "        [0.1092],\n",
      "        [0.1104],\n",
      "        [0.1112]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1010],\n",
      "        [0.1026],\n",
      "        [0.1007],\n",
      "        [0.1042],\n",
      "        [0.0979],\n",
      "        [0.0972],\n",
      "        [0.0968],\n",
      "        [0.0983],\n",
      "        [0.0971],\n",
      "        [0.1043]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0840],\n",
      "        [0.0991],\n",
      "        [0.0907],\n",
      "        [0.0843],\n",
      "        [0.0991],\n",
      "        [0.1045],\n",
      "        [0.1074],\n",
      "        [0.1092],\n",
      "        [0.1105],\n",
      "        [0.1113]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0818],\n",
      "        [0.0827],\n",
      "        [0.0952],\n",
      "        [0.1005],\n",
      "        [0.1035],\n",
      "        [0.1054],\n",
      "        [0.1067],\n",
      "        [0.1075],\n",
      "        [0.1081],\n",
      "        [0.1085]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0959],\n",
      "        [0.1058],\n",
      "        [0.0995],\n",
      "        [0.0913],\n",
      "        [0.0927],\n",
      "        [0.0929],\n",
      "        [0.1010],\n",
      "        [0.1048],\n",
      "        [0.1072],\n",
      "        [0.1089]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0793],\n",
      "        [0.1016],\n",
      "        [0.0952],\n",
      "        [0.0942],\n",
      "        [0.0881],\n",
      "        [0.0924],\n",
      "        [0.1161],\n",
      "        [0.1152],\n",
      "        [0.1070],\n",
      "        [0.1110]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0922],\n",
      "        [0.0925],\n",
      "        [0.1007],\n",
      "        [0.1103],\n",
      "        [0.1069],\n",
      "        [0.0965],\n",
      "        [0.1086],\n",
      "        [0.1012],\n",
      "        [0.0954],\n",
      "        [0.0958]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0793],\n",
      "        [0.0827],\n",
      "        [0.0962],\n",
      "        [0.1014],\n",
      "        [0.1041],\n",
      "        [0.1057],\n",
      "        [0.1068],\n",
      "        [0.1075],\n",
      "        [0.1080],\n",
      "        [0.1083]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1061],\n",
      "        [0.0915],\n",
      "        [0.0878],\n",
      "        [0.1072],\n",
      "        [0.0924],\n",
      "        [0.0985],\n",
      "        [0.1012],\n",
      "        [0.1034],\n",
      "        [0.1052],\n",
      "        [0.1066]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0893],\n",
      "        [0.0913],\n",
      "        [0.0962],\n",
      "        [0.1066],\n",
      "        [0.1134],\n",
      "        [0.1058],\n",
      "        [0.1097],\n",
      "        [0.0976],\n",
      "        [0.0978],\n",
      "        [0.0922]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0845],\n",
      "        [0.0886],\n",
      "        [0.0869],\n",
      "        [0.0819],\n",
      "        [0.0995],\n",
      "        [0.1064],\n",
      "        [0.1102],\n",
      "        [0.1126],\n",
      "        [0.1141],\n",
      "        [0.1151]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0794],\n",
      "        [0.0795],\n",
      "        [0.0916],\n",
      "        [0.1011],\n",
      "        [0.1047],\n",
      "        [0.1068],\n",
      "        [0.1082],\n",
      "        [0.1090],\n",
      "        [0.1096],\n",
      "        [0.1101]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0914],\n",
      "        [0.0930],\n",
      "        [0.0895],\n",
      "        [0.0891],\n",
      "        [0.0950],\n",
      "        [0.1098],\n",
      "        [0.0955],\n",
      "        [0.1063],\n",
      "        [0.1132],\n",
      "        [0.1171]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0833],\n",
      "        [0.0913],\n",
      "        [0.0944],\n",
      "        [0.0955],\n",
      "        [0.1052],\n",
      "        [0.0997],\n",
      "        [0.1160],\n",
      "        [0.1049],\n",
      "        [0.0948],\n",
      "        [0.1149]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0733],\n",
      "        [0.0981],\n",
      "        [0.1010],\n",
      "        [0.1024],\n",
      "        [0.1033],\n",
      "        [0.1039],\n",
      "        [0.1042],\n",
      "        [0.1045],\n",
      "        [0.1046],\n",
      "        [0.1047]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0991],\n",
      "        [0.0822],\n",
      "        [0.1046],\n",
      "        [0.0974],\n",
      "        [0.0932],\n",
      "        [0.1022],\n",
      "        [0.0956],\n",
      "        [0.0988],\n",
      "        [0.1100],\n",
      "        [0.1168]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0986],\n",
      "        [0.0973],\n",
      "        [0.0995],\n",
      "        [0.0985],\n",
      "        [0.0889],\n",
      "        [0.0865],\n",
      "        [0.1009],\n",
      "        [0.1067],\n",
      "        [0.1103],\n",
      "        [0.1128]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0864],\n",
      "        [0.0871],\n",
      "        [0.0960],\n",
      "        [0.0936],\n",
      "        [0.0880],\n",
      "        [0.0886],\n",
      "        [0.1060],\n",
      "        [0.1143],\n",
      "        [0.1187],\n",
      "        [0.1213]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1031],\n",
      "        [0.0923],\n",
      "        [0.0943],\n",
      "        [0.0963],\n",
      "        [0.1057],\n",
      "        [0.1068],\n",
      "        [0.1028],\n",
      "        [0.1087],\n",
      "        [0.1001],\n",
      "        [0.0900]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0854],\n",
      "        [0.1004],\n",
      "        [0.0918],\n",
      "        [0.0805],\n",
      "        [0.0866],\n",
      "        [0.1033],\n",
      "        [0.1092],\n",
      "        [0.1124],\n",
      "        [0.1144],\n",
      "        [0.1158]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0750],\n",
      "        [0.0808],\n",
      "        [0.0844],\n",
      "        [0.0931],\n",
      "        [0.0956],\n",
      "        [0.1062],\n",
      "        [0.1122],\n",
      "        [0.1157],\n",
      "        [0.1178],\n",
      "        [0.1192]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0909],\n",
      "        [0.0951],\n",
      "        [0.0957],\n",
      "        [0.1009],\n",
      "        [0.1173],\n",
      "        [0.1018],\n",
      "        [0.1015],\n",
      "        [0.1084],\n",
      "        [0.1014],\n",
      "        [0.0869]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0756],\n",
      "        [0.0698],\n",
      "        [0.0924],\n",
      "        [0.1013],\n",
      "        [0.1059],\n",
      "        [0.1086],\n",
      "        [0.1102],\n",
      "        [0.1113],\n",
      "        [0.1121],\n",
      "        [0.1127]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0753],\n",
      "        [0.0804],\n",
      "        [0.0933],\n",
      "        [0.0999],\n",
      "        [0.1039],\n",
      "        [0.1066],\n",
      "        [0.1084],\n",
      "        [0.1098],\n",
      "        [0.1108],\n",
      "        [0.1116]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.1083],\n",
      "        [0.1038],\n",
      "        [0.0971],\n",
      "        [0.0877],\n",
      "        [0.0909],\n",
      "        [0.0908],\n",
      "        [0.1015],\n",
      "        [0.1009],\n",
      "        [0.1123],\n",
      "        [0.1066]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0753],\n",
      "        [0.0694],\n",
      "        [0.0924],\n",
      "        [0.1014],\n",
      "        [0.1060],\n",
      "        [0.1087],\n",
      "        [0.1104],\n",
      "        [0.1115],\n",
      "        [0.1122],\n",
      "        [0.1128]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0741],\n",
      "        [0.0830],\n",
      "        [0.0937],\n",
      "        [0.0997],\n",
      "        [0.1036],\n",
      "        [0.1063],\n",
      "        [0.1082],\n",
      "        [0.1096],\n",
      "        [0.1106],\n",
      "        [0.1113]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0900],\n",
      "        [0.0987],\n",
      "        [0.0967],\n",
      "        [0.1006],\n",
      "        [0.1110],\n",
      "        [0.1122],\n",
      "        [0.1015],\n",
      "        [0.0942],\n",
      "        [0.0924],\n",
      "        [0.1028]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0879],\n",
      "        [0.0946],\n",
      "        [0.0981],\n",
      "        [0.1002],\n",
      "        [0.1016],\n",
      "        [0.1025],\n",
      "        [0.1032],\n",
      "        [0.1037],\n",
      "        [0.1040],\n",
      "        [0.1043]], grad_fn=<SoftmaxBackward>)\n",
      "epoch 1/1, loss: 0.40181756019592285\n"
     ]
    }
   ],
   "source": [
    "word_encoder = wordEncoder(VOCAB_SIZE, HIDDEN_SIZE, EMBEDDING_DIM).to(DEVICE)\n",
    "sent_encoder = sentEncoder(HIDDEN_SIZE * 2).to(DEVICE)\n",
    "model = HAN(word_encoder, sent_encoder, NUM_CLASSES, DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.NLLLoss()\n",
    "losses = []\n",
    "weights = []\n",
    "\n",
    "for i in tqdm(range(NUM_EPOCHS)):\n",
    "    current_loss = 0\n",
    "    for j in range(len(tweets[:50])):\n",
    "        tweet, score = torch.tensor(tweets[j], dtype = torch.long).to(DEVICE), torch.tensor(sent_scores[j]).to(DEVICE)\n",
    "        word_weights, sent_weights, output = model(tweet)\n",
    "        optimizer.zero_grad()\n",
    "        #current_loss += criterion(output.unsqueeze(0), score.unsqueeze(0))\n",
    "        current_loss = criterion(output.unsqueeze(0), score.unsqueeze(0))\n",
    "        current_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"epoch {i+1}/{NUM_EPOCHS}, loss: {current_loss}\")\n",
    "    losses.append(current_loss.item()/(j+1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3796, 1081, 4739, 5202, 4182, 4182, 4182, 4182, 4182, 4182],\n",
      "        [5084, 3871, 5270,  711, 5173, 3668,  777,  968, 4182, 4182],\n",
      "        [1615, 4962, 3188, 3808,  893, 2970, 4182, 4182, 4182, 4182],\n",
      "        [3244,  293, 4182, 4182, 4182, 4182, 4182, 4182, 4182, 4182]])\n",
      "Class: 3\n",
      "~~~ RESULTS ~~~\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0771],\n",
      "        [0.0996],\n",
      "        [0.0989],\n",
      "        [0.0870],\n",
      "        [0.0989],\n",
      "        [0.1040],\n",
      "        [0.1067],\n",
      "        [0.1083],\n",
      "        [0.1094],\n",
      "        [0.1101]])\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0886],\n",
      "        [0.0913],\n",
      "        [0.0833],\n",
      "        [0.0881],\n",
      "        [0.1001],\n",
      "        [0.0974],\n",
      "        [0.0992],\n",
      "        [0.1079],\n",
      "        [0.1194],\n",
      "        [0.1247]])\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0879],\n",
      "        [0.0874],\n",
      "        [0.0925],\n",
      "        [0.0901],\n",
      "        [0.0887],\n",
      "        [0.0811],\n",
      "        [0.1070],\n",
      "        [0.1172],\n",
      "        [0.1225],\n",
      "        [0.1256]])\n",
      "\n",
      "word_weights\n",
      "tensor([[0.0721],\n",
      "        [0.0747],\n",
      "        [0.0947],\n",
      "        [0.1023],\n",
      "        [0.1061],\n",
      "        [0.1082],\n",
      "        [0.1095],\n",
      "        [0.1103],\n",
      "        [0.1108],\n",
      "        [0.1112]])\n",
      "tensor([-3.4175, -3.8605, -2.3675, -0.4045, -2.2237, -2.5651])\n",
      "[tensor([[0.0771],\n",
      "        [0.0996],\n",
      "        [0.0989],\n",
      "        [0.0870],\n",
      "        [0.0989],\n",
      "        [0.1040],\n",
      "        [0.1067],\n",
      "        [0.1083],\n",
      "        [0.1094],\n",
      "        [0.1101]]), tensor([[0.0886],\n",
      "        [0.0913],\n",
      "        [0.0833],\n",
      "        [0.0881],\n",
      "        [0.1001],\n",
      "        [0.0974],\n",
      "        [0.0992],\n",
      "        [0.1079],\n",
      "        [0.1194],\n",
      "        [0.1247]]), tensor([[0.0879],\n",
      "        [0.0874],\n",
      "        [0.0925],\n",
      "        [0.0901],\n",
      "        [0.0887],\n",
      "        [0.0811],\n",
      "        [0.1070],\n",
      "        [0.1172],\n",
      "        [0.1225],\n",
      "        [0.1256]]), tensor([[0.0721],\n",
      "        [0.0747],\n",
      "        [0.0947],\n",
      "        [0.1023],\n",
      "        [0.1061],\n",
      "        [0.1082],\n",
      "        [0.1095],\n",
      "        [0.1103],\n",
      "        [0.1108],\n",
      "        [0.1112]])]\n",
      "tensor([[0.2650],\n",
      "        [0.2340],\n",
      "        [0.2190],\n",
      "        [0.2820]])\n",
      "~~~ Prediction ~~~\n",
      "Class: 3\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    tweet, score = torch.tensor(tweets[50], dtype = torch.long).to(DEVICE), torch.tensor(sent_scores[j]).to(DEVICE)\n",
    "    print(tweet)\n",
    "    print(\"Class:\", score.item())\n",
    "\n",
    "    print(\"~~~ RESULTS ~~~\")\n",
    "    word_weights, sent_weights, output = model(tweet)\n",
    "    print(output)\n",
    "    print(word_weights)\n",
    "    print(sent_weights)\n",
    "    print(\"~~~ Prediction ~~~\")\n",
    "    _, idx = torch.max(output, 0)\n",
    "    print(\"Class:\",idx.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing a rover nasaames <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "it uses same tech as self driving cars <pad> <pad>\n",
      "stateofnasa nasasocial nasa û_ http t <pad> <pad> <pad> <pad>\n",
      "co pin2j8fusj <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Correct: 3\n"
     ]
    }
   ],
   "source": [
    "for t in tweet:\n",
    "    t = t.numpy()\n",
    "    sent = \" \".join([unique_tokens[w] for w in t])\n",
    "    print(sent)\n",
    "\n",
    "if score.item() == idx.item():\n",
    "    print(f\"Correct: {score.item()}\")\n",
    "else:\n",
    "    print(f\"Truth: {score.item()}, Predicted:{idx.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ~ ~ ~ T E S T S  ~ ~ ~"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get a document"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3569, 2433, 1940, 5684, 6000, 1145, 480, 4786, 2378, 3436], [4985, 92, 1147, 3515, 2246, 5117, 6184, 6184, 6184, 6184], [4776, 2312, 6184, 6184, 6184, 6184, 6184, 6184, 6184, 6184], [2718, 2051, 6184, 6184, 6184, 6184, 6184, 6184, 6184, 6184]]\n",
      "[3569, 2433, 1940, 5684, 6000, 1145, 480, 4786, 2378, 3436]\n",
      "what intersections would look like in a world with only\n",
      "\n",
      "[4985, 92, 1147, 3515, 2246, 5117, 6184, 6184, 6184, 6184]\n",
      "i literally winced the whole time <pad> <pad> <pad> <pad>\n",
      "\n",
      "[4776, 2312, 6184, 6184, 6184, 6184, 6184, 6184, 6184, 6184]\n",
      "http t <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "[2718, 2051, 6184, 6184, 6184, 6184, 6184, 6184, 6184, 6184]\n",
      "co 1ekgcqdm <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Total of sentences in doc: 4\n",
      "Score: 2\n"
     ]
    }
   ],
   "source": [
    "tweet_id = 9\n",
    "print(tweets[tweet_id])\n",
    "doc = tweets[tweet_id]\n",
    "\n",
    "\n",
    "\n",
    "for t in doc:\n",
    "    print(t)\n",
    "    sent = \" \".join([unique_tokens[w] for w in t])\n",
    "    print(sent)\n",
    "    print()\n",
    "\n",
    "print(\"Total of sentences in doc:\",len(doc))\n",
    "\n",
    "print(\"Score:\", sent_scores[tweet_id])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform the document and its score into a tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "tensor([[3569, 2433, 1940, 5684, 6000, 1145,  480, 4786, 2378, 3436],\n",
      "        [4985,   92, 1147, 3515, 2246, 5117, 6184, 6184, 6184, 6184],\n",
      "        [4776, 2312, 6184, 6184, 6184, 6184, 6184, 6184, 6184, 6184],\n",
      "        [2718, 2051, 6184, 6184, 6184, 6184, 6184, 6184, 6184, 6184]])\n",
      "torch.LongTensor\n",
      "torch.Size([4, 10])\n",
      "\n",
      "tensor(3)\n",
      "torch.LongTensor\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(\"Device:\", DEVICE)\n",
    "document = torch.tensor(doc, dtype = torch.long).to(DEVICE)\n",
    "score = torch.tensor(sent_scores[j]).to(DEVICE)\n",
    "\n",
    "print(document)\n",
    "print(document.type())\n",
    "print(document.shape)\n",
    "print()\n",
    "print(score)\n",
    "print(score.type())\n",
    "print(score.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing encoders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "word_encoder = wordEncoder(VOCAB_SIZE, HIDDEN_SIZE, EMBEDDING_DIM).to(DEVICE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Embedding(6330, 30)\n",
      "6330\n",
      "GRU(30, 16, bidirectional=True)\n",
      "wordEncoder(\n",
      "  (embedding): Embedding(6330, 30)\n",
      "  (gru): GRU(30, 16, bidirectional=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(word_encoder.hidden_size)\n",
    "print(word_encoder.embedding)\n",
    "print(word_encoder.vocab_size)\n",
    "print(word_encoder.gru)\n",
    "print(word_encoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "word_attention_weights = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([4, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "sentenc_out = torch.zeros((document.size(0), 2, sent_encoder.hidden_size)).to(DEVICE)\n",
    "print(sentenc_out)\n",
    "print(sentenc_out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0 in document\n",
      "~~~ Creating a new wordenc_out\n",
      "word tensor(3569)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(2433)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(1940)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(5684)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6000)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(1145)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(480)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(4786)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(2378)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(3436)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "#_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_#\n",
      "Continuing in sentence 0\n",
      "Sentence 1 in document\n",
      "~~~ Creating a new wordenc_out\n",
      "word tensor(4985)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(92)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(1147)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(3515)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(2246)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(5117)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "#_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_#\n",
      "Continuing in sentence 1\n",
      "Sentence 2 in document\n",
      "~~~ Creating a new wordenc_out\n",
      "word tensor(4776)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(2312)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "#_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_#\n",
      "Continuing in sentence 2\n",
      "Sentence 3 in document\n",
      "~~~ Creating a new wordenc_out\n",
      "word tensor(2718)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(2051)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "word tensor(6184)\n",
      "torch.Size([2, 1, 16])\n",
      "##################################################\n",
      "#_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_##_#\n",
      "Continuing in sentence 3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for i in range(document.size(0)):\n",
    "    print(f\"Sentence {i} in document\")\n",
    "    sent = document[i]\n",
    "    print(\"~~~ Creating a new wordenc_out\")\n",
    "    wordenc_out = torch.zeros((sent.size(0), 2, word_encoder.hidden_size)).to(DEVICE)\n",
    "    h0_word = torch.zeros(2, 1, word_encoder.hidden_size, dtype = torch.float).to(DEVICE)\n",
    "    # iterate on words\n",
    "    for j in range(sent.size(0)):\n",
    "        print(\"word\", sent[j])\n",
    "        _, h0_word = word_encoder(sent[j], h0_word)\n",
    "\n",
    "        #print(_)\n",
    "        #print(_.shape)\n",
    "        #print(\"#\"*50)\n",
    "        #print(h0_word)\n",
    "        print(h0_word.shape)\n",
    "        print(\"#\" * 50)\n",
    "        wordenc_out[j] = h0_word.squeeze()\n",
    "        #print(wordenc_out)\n",
    "    print(\"#_#\" * 50)\n",
    "    print(f\"Continuing in sentence {i}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32])\n"
     ]
    }
   ],
   "source": [
    "    wordenc_out = wordenc_out.view(wordenc_out.size(0), -1)\n",
    "    print(wordenc_out.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "    # word-level attention\n",
    "    word_attention = nn.Linear(word_encoder.hidden_size*2, word_encoder.hidden_size*2)\n",
    "    u_w = nn.Linear(word_encoder.hidden_size*2, 1, bias = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1042, -0.2369, -0.3620, -0.1506,  0.4012, -0.2512, -0.1348, -0.0056,\n",
      "          0.0571,  0.1853, -0.1059,  0.0846,  0.0639,  0.0702,  0.2185,  0.3496,\n",
      "         -0.0015, -0.2413,  0.1023, -0.2145, -0.3328, -0.2059,  0.1359, -0.0793,\n",
      "         -0.0425, -0.1120, -0.1226,  0.3884,  0.2770, -0.2538,  0.1783,  0.0865],\n",
      "        [ 0.3290,  0.0959, -0.3581, -0.1266,  0.5102,  0.0231, -0.2772, -0.0332,\n",
      "          0.0640,  0.0406, -0.0917, -0.0729, -0.1020,  0.0492,  0.2190,  0.1950,\n",
      "         -0.1644, -0.1984,  0.0360, -0.3093, -0.1349, -0.1019, -0.0518, -0.0663,\n",
      "         -0.3275, -0.0023, -0.1576,  0.2930,  0.2203, -0.0012,  0.0010,  0.2678],\n",
      "        [ 0.1045, -0.2553, -0.3143,  0.0171,  0.1703, -0.1433, -0.4102,  0.1004,\n",
      "         -0.0268, -0.1532, -0.0734,  0.0041, -0.0481,  0.1687,  0.1280, -0.0411,\n",
      "         -0.3815, -0.0985, -0.1021,  0.0643, -0.0470,  0.0625,  0.0605, -0.0558,\n",
      "          0.1663,  0.0799, -0.0094,  0.0759,  0.2678, -0.1145,  0.1214,  0.3028],\n",
      "        [ 0.0122, -0.3534, -0.3142,  0.0981,  0.0214, -0.2298, -0.4152,  0.2553,\n",
      "         -0.0977, -0.2582, -0.1121,  0.0398, -0.0333,  0.2604,  0.1554, -0.1618,\n",
      "         -0.4192, -0.1091, -0.1127,  0.2238, -0.0045,  0.1602,  0.1107,  0.0299,\n",
      "          0.3484,  0.1082,  0.0329,  0.0664,  0.2555, -0.1516,  0.1972,  0.3183],\n",
      "        [-0.0312, -0.3890, -0.3254,  0.1335, -0.0425, -0.2955, -0.4039,  0.3764,\n",
      "         -0.1469, -0.3016, -0.1542,  0.0595, -0.0067,  0.3136,  0.1757, -0.2343,\n",
      "         -0.4248, -0.1507, -0.0918,  0.3133,  0.0064,  0.2154,  0.1386,  0.0949,\n",
      "          0.4324,  0.1228,  0.0513,  0.0824,  0.2398, -0.1728,  0.2343,  0.3301],\n",
      "        [-0.0542, -0.4031, -0.3374,  0.1504, -0.0656, -0.3411, -0.3895,  0.4605,\n",
      "         -0.1749, -0.3157, -0.1841,  0.0744,  0.0218,  0.3481,  0.1849, -0.2755,\n",
      "         -0.4213, -0.1935, -0.0649,  0.3627,  0.0078,  0.2447,  0.1533,  0.1376,\n",
      "          0.4729,  0.1334,  0.0603,  0.0988,  0.2257, -0.1875,  0.2516,  0.3341],\n",
      "        [-0.0681, -0.4097, -0.3467,  0.1596, -0.0712, -0.3718, -0.3763,  0.5162,\n",
      "         -0.1891, -0.3181, -0.2019,  0.0865,  0.0466,  0.3722,  0.1891, -0.2985,\n",
      "         -0.4149, -0.2295, -0.0417,  0.3889,  0.0072,  0.2588,  0.1605,  0.1647,\n",
      "          0.4924,  0.1419,  0.0653,  0.1113,  0.2151, -0.1991,  0.2605,  0.3330],\n",
      "        [-0.0775, -0.4135, -0.3530,  0.1652, -0.0701, -0.3923, -0.3655,  0.5525,\n",
      "         -0.1955, -0.3166, -0.2115,  0.0958,  0.0667,  0.3896,  0.1913, -0.3115,\n",
      "         -0.4080, -0.2575, -0.0244,  0.4028,  0.0066,  0.2646,  0.1638,  0.1815,\n",
      "          0.5022,  0.1488,  0.0684,  0.1200,  0.2078, -0.2084,  0.2658,  0.3298],\n",
      "        [-0.0842, -0.4164, -0.3570,  0.1691, -0.0672, -0.4065, -0.3573,  0.5762,\n",
      "         -0.1980, -0.3142, -0.2163,  0.1029,  0.0825,  0.4021,  0.1926, -0.3190,\n",
      "         -0.4016, -0.2786, -0.0124,  0.4105,  0.0066,  0.2663,  0.1652,  0.1923,\n",
      "          0.5074,  0.1544,  0.0702,  0.1257,  0.2030, -0.2160,  0.2693,  0.3263],\n",
      "        [-0.0892, -0.4189, -0.3594,  0.1721, -0.0643, -0.4167, -0.3511,  0.5918,\n",
      "         -0.1985, -0.3120, -0.2187,  0.1080,  0.0950,  0.4111,  0.1934, -0.3234,\n",
      "         -0.3962, -0.2943, -0.0044,  0.4150,  0.0072,  0.2663,  0.1660,  0.1992,\n",
      "          0.5105,  0.1588,  0.0712,  0.1296,  0.2000, -0.2222,  0.2720,  0.3233]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "torch.Size([10, 32])\n"
     ]
    }
   ],
   "source": [
    "    x = word_attention(wordenc_out)\n",
    "    print(x)\n",
    "    print(x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1038, -0.2326, -0.3469, -0.1494,  0.3810, -0.2461, -0.1340, -0.0056,\n",
      "          0.0571,  0.1832, -0.1055,  0.0844,  0.0638,  0.0701,  0.2151,  0.3360,\n",
      "         -0.0015, -0.2367,  0.1019, -0.2113, -0.3211, -0.2031,  0.1350, -0.0792,\n",
      "         -0.0425, -0.1116, -0.1220,  0.3700,  0.2702, -0.2485,  0.1764,  0.0863],\n",
      "        [ 0.3177,  0.0956, -0.3435, -0.1259,  0.4701,  0.0231, -0.2703, -0.0332,\n",
      "          0.0639,  0.0405, -0.0914, -0.0727, -0.1017,  0.0492,  0.2156,  0.1925,\n",
      "         -0.1630, -0.1958,  0.0360, -0.2998, -0.1341, -0.1015, -0.0518, -0.0662,\n",
      "         -0.3162, -0.0023, -0.1563,  0.2849,  0.2168, -0.0012,  0.0010,  0.2616],\n",
      "        [ 0.1042, -0.2499, -0.3044,  0.0171,  0.1686, -0.1423, -0.3887,  0.1000,\n",
      "         -0.0268, -0.1520, -0.0733,  0.0041, -0.0480,  0.1671,  0.1273, -0.0411,\n",
      "         -0.3640, -0.0981, -0.1018,  0.0642, -0.0469,  0.0624,  0.0605, -0.0558,\n",
      "          0.1648,  0.0797, -0.0094,  0.0758,  0.2616, -0.1140,  0.1208,  0.2939],\n",
      "        [ 0.0122, -0.3394, -0.3043,  0.0978,  0.0214, -0.2259, -0.3929,  0.2499,\n",
      "         -0.0974, -0.2526, -0.1116,  0.0398, -0.0333,  0.2547,  0.1542, -0.1604,\n",
      "         -0.3962, -0.1086, -0.1122,  0.2201, -0.0045,  0.1588,  0.1103,  0.0299,\n",
      "          0.3350,  0.1078,  0.0328,  0.0663,  0.2500, -0.1505,  0.1947,  0.3080],\n",
      "        [-0.0312, -0.3705, -0.3144,  0.1327, -0.0424, -0.2872, -0.3833,  0.3596,\n",
      "         -0.1459, -0.2928, -0.1530,  0.0594, -0.0067,  0.3037,  0.1739, -0.2301,\n",
      "         -0.4010, -0.1495, -0.0916,  0.3034,  0.0064,  0.2121,  0.1378,  0.0946,\n",
      "          0.4074,  0.1222,  0.0512,  0.0822,  0.2353, -0.1711,  0.2301,  0.3186],\n",
      "        [-0.0541, -0.3826, -0.3251,  0.1493, -0.0655, -0.3285, -0.3709,  0.4305,\n",
      "         -0.1731, -0.3056, -0.1820,  0.0743,  0.0218,  0.3346,  0.1828, -0.2687,\n",
      "         -0.3980, -0.1912, -0.0648,  0.3476,  0.0078,  0.2399,  0.1521,  0.1368,\n",
      "          0.4405,  0.1327,  0.0602,  0.0985,  0.2219, -0.1854,  0.2465,  0.3222],\n",
      "        [-0.0680, -0.3882, -0.3334,  0.1582, -0.0711, -0.3555, -0.3595,  0.4748,\n",
      "         -0.1869, -0.3078, -0.1992,  0.0862,  0.0466,  0.3559,  0.1868, -0.2899,\n",
      "         -0.3926, -0.2256, -0.0417,  0.3704,  0.0072,  0.2531,  0.1591,  0.1632,\n",
      "          0.4562,  0.1410,  0.0652,  0.1109,  0.2119, -0.1965,  0.2548,  0.3212],\n",
      "        [-0.0773, -0.3915, -0.3390,  0.1637, -0.0700, -0.3734, -0.3501,  0.5024,\n",
      "         -0.1931, -0.3064, -0.2084,  0.0955,  0.0666,  0.3710,  0.1890, -0.3018,\n",
      "         -0.3868, -0.2520, -0.0244,  0.3824,  0.0066,  0.2585,  0.1623,  0.1796,\n",
      "          0.4638,  0.1477,  0.0682,  0.1194,  0.2049, -0.2054,  0.2597,  0.3184],\n",
      "        [-0.0840, -0.3939, -0.3426,  0.1675, -0.0671, -0.3855, -0.3428,  0.5199,\n",
      "         -0.1954, -0.3043, -0.2130,  0.1025,  0.0823,  0.3818,  0.1902, -0.3086,\n",
      "         -0.3814, -0.2716, -0.0124,  0.3889,  0.0066,  0.2602,  0.1637,  0.1899,\n",
      "          0.4679,  0.1532,  0.0701,  0.1250,  0.2003, -0.2127,  0.2630,  0.3152],\n",
      "        [-0.0890, -0.3960, -0.3447,  0.1704, -0.0642, -0.3941, -0.3374,  0.5312,\n",
      "         -0.1960, -0.3022, -0.2152,  0.1076,  0.0947,  0.3894,  0.1911, -0.3126,\n",
      "         -0.3767, -0.2861, -0.0044,  0.3927,  0.0072,  0.2602,  0.1645,  0.1967,\n",
      "          0.4704,  0.1575,  0.0711,  0.1289,  0.1973, -0.2186,  0.2655,  0.3125]],\n",
      "       grad_fn=<TanhBackward>)\n",
      "torch.Size([10, 32])\n"
     ]
    }
   ],
   "source": [
    "    u_word = torch.tanh(x)\n",
    "    print(u_word)\n",
    "    print(u_word.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0833],\n",
      "        [-0.0854],\n",
      "        [-0.0749],\n",
      "        [-0.0837],\n",
      "        [-0.0884],\n",
      "        [-0.0896],\n",
      "        [-0.0891],\n",
      "        [-0.0878],\n",
      "        [-0.0865],\n",
      "        [-0.0854]], grad_fn=<MmBackward>)\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "    y = u_w(u_word)\n",
    "    print(y)\n",
    "    print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "    softmax = nn.Softmax(dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1002],\n",
      "        [0.1000],\n",
      "        [0.1011],\n",
      "        [0.1002],\n",
      "        [0.0997],\n",
      "        [0.0996],\n",
      "        [0.0996],\n",
      "        [0.0998],\n",
      "        [0.0999],\n",
      "        [0.1000]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([10, 1])\n",
      "tensor([1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "    word_weights = softmax(y)\n",
    "    print(word_weights)\n",
    "    print(word_weights.shape)\n",
    "    print(word_weights.sum(dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[tensor([[0.0962],\n",
      "        [0.0797],\n",
      "        [0.0865],\n",
      "        [0.0958],\n",
      "        [0.1019],\n",
      "        [0.1054],\n",
      "        [0.1074],\n",
      "        [0.1085],\n",
      "        [0.1091],\n",
      "        [0.1095]], grad_fn=<SoftmaxBackward>)]\n"
     ]
    }
   ],
   "source": [
    "    word_attention_weights.append(word_weights)\n",
    "    print(len(word_attention_weights))\n",
    "    print(word_attention_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentence Representation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0137, -0.3047, -0.3298,  0.0779,  0.0664, -0.2713, -0.3330,  0.3125,\n",
      "        -0.1091, -0.1997, -0.1551,  0.0580,  0.0185,  0.2675,  0.1825, -0.1381,\n",
      "        -0.3260, -0.2014, -0.0316,  0.1954, -0.0466,  0.1398,  0.1192,  0.0787,\n",
      "         0.2843,  0.0927,  0.0130,  0.1462,  0.2271, -0.1703,  0.2011,  0.2857],\n",
      "       grad_fn=<SumBackward1>)\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "    sent_summ_vector = (u_word * word_weights).sum(axis=0)\n",
    "    print(sent_summ_vector)\n",
    "    print(sent_summ_vector.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the sentence Representation, do the sentence encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentEncoder(\n",
      "  (gru): GRU(32, 32, bidirectional=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "    sent_encoder = sentEncoder(HIDDEN_SIZE * 2).to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentEncoder(\n",
      "  (gru): GRU(32, 32, bidirectional=True)\n",
      ")\n",
      "32\n",
      "GRU(32, 32, bidirectional=True)\n",
      "sentEncoder(\n",
      "  (gru): GRU(32, 32, bidirectional=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "    print(sent_encoder)\n",
    "    print(sent_encoder.hidden_size)\n",
    "    print(sent_encoder.gru)\n",
    "    print(sent_encoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "    h0_sent = torch.zeros(2, 1, sent_encoder.hidden_size, dtype=float).to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)\n",
      "torch.Size([2, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "    print(h0_sent)\n",
    "    print(h0_sent.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0205e-02, -4.0110e-02, -3.1047e-02, -1.0309e-01,  6.1025e-02,\n",
      "           9.7053e-02,  2.3997e-02, -1.1006e-01, -7.9249e-02,  1.4116e-01,\n",
      "          -1.1850e-01,  7.6383e-03, -6.2079e-02, -5.2774e-02,  4.4817e-02,\n",
      "          -4.2933e-02,  1.0216e-02, -8.4887e-02,  5.0992e-02, -8.3613e-02,\n",
      "           2.5807e-02,  1.3175e-01,  2.9545e-02,  4.9353e-02, -2.2592e-02,\n",
      "          -6.6765e-02,  5.2205e-02,  1.1331e-01, -4.4384e-02,  9.8815e-03,\n",
      "           9.3066e-02, -2.8365e-02, -1.3375e-01, -1.3185e-02,  2.2988e-02,\n",
      "          -1.7301e-02,  1.2917e-01,  1.4210e-01, -9.3730e-02,  6.1013e-02,\n",
      "          -4.9668e-02, -5.0703e-02,  9.2445e-02, -9.9274e-02,  8.8130e-02,\n",
      "           6.3865e-02,  4.4382e-02, -7.2982e-02,  5.8645e-02, -6.2847e-02,\n",
      "          -1.4688e-02,  7.2641e-02, -3.0726e-02, -7.3274e-02,  1.0716e-01,\n",
      "           2.6822e-03, -1.5771e-01,  2.1897e-02,  3.5497e-02, -3.3932e-02,\n",
      "           6.1915e-02, -1.2432e-04, -9.6328e-02,  7.9967e-02]]],\n",
      "       grad_fn=<CatBackward>)\n",
      "torch.Size([1, 1, 64])\n",
      "\n",
      "tensor([[[ 1.0205e-02, -4.0110e-02, -3.1047e-02, -1.0309e-01,  6.1025e-02,\n",
      "           9.7053e-02,  2.3997e-02, -1.1006e-01, -7.9249e-02,  1.4116e-01,\n",
      "          -1.1850e-01,  7.6383e-03, -6.2079e-02, -5.2774e-02,  4.4817e-02,\n",
      "          -4.2933e-02,  1.0216e-02, -8.4887e-02,  5.0992e-02, -8.3613e-02,\n",
      "           2.5807e-02,  1.3175e-01,  2.9545e-02,  4.9353e-02, -2.2592e-02,\n",
      "          -6.6765e-02,  5.2205e-02,  1.1331e-01, -4.4384e-02,  9.8815e-03,\n",
      "           9.3066e-02, -2.8365e-02]],\n",
      "\n",
      "        [[-1.3375e-01, -1.3185e-02,  2.2988e-02, -1.7301e-02,  1.2917e-01,\n",
      "           1.4210e-01, -9.3730e-02,  6.1013e-02, -4.9668e-02, -5.0703e-02,\n",
      "           9.2445e-02, -9.9274e-02,  8.8130e-02,  6.3865e-02,  4.4382e-02,\n",
      "          -7.2982e-02,  5.8645e-02, -6.2847e-02, -1.4688e-02,  7.2641e-02,\n",
      "          -3.0726e-02, -7.3274e-02,  1.0716e-01,  2.6822e-03, -1.5771e-01,\n",
      "           2.1897e-02,  3.5497e-02, -3.3932e-02,  6.1915e-02, -1.2432e-04,\n",
      "          -9.6328e-02,  7.9967e-02]]], grad_fn=<StackBackward>)\n",
      "torch.Size([2, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "    _, h0_sent = sent_encoder(sent_summ_vector, h0_sent)\n",
    "    print(_)\n",
    "    print(_.shape)\n",
    "    print()\n",
    "    print(h0_sent)\n",
    "    print(h0_sent.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Almacena h0 de cada sentencia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.0205e-02, -4.0110e-02, -3.1047e-02, -1.0309e-01,  6.1025e-02,\n",
      "           9.7053e-02,  2.3997e-02, -1.1006e-01, -7.9249e-02,  1.4116e-01,\n",
      "          -1.1850e-01,  7.6383e-03, -6.2079e-02, -5.2774e-02,  4.4817e-02,\n",
      "          -4.2933e-02,  1.0216e-02, -8.4887e-02,  5.0992e-02, -8.3613e-02,\n",
      "           2.5807e-02,  1.3175e-01,  2.9545e-02,  4.9353e-02, -2.2592e-02,\n",
      "          -6.6765e-02,  5.2205e-02,  1.1331e-01, -4.4384e-02,  9.8815e-03,\n",
      "           9.3066e-02, -2.8365e-02],\n",
      "         [-1.3375e-01, -1.3185e-02,  2.2988e-02, -1.7301e-02,  1.2917e-01,\n",
      "           1.4210e-01, -9.3730e-02,  6.1013e-02, -4.9668e-02, -5.0703e-02,\n",
      "           9.2445e-02, -9.9274e-02,  8.8130e-02,  6.3865e-02,  4.4382e-02,\n",
      "          -7.2982e-02,  5.8645e-02, -6.2847e-02, -1.4688e-02,  7.2641e-02,\n",
      "          -3.0726e-02, -7.3274e-02,  1.0716e-01,  2.6822e-03, -1.5771e-01,\n",
      "           2.1897e-02,  3.5497e-02, -3.3932e-02,  6.1915e-02, -1.2432e-04,\n",
      "          -9.6328e-02,  7.9967e-02]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "    print(h0_sent.squeeze().shape)\n",
    "    sentenc_out[i] = h0_sent.squeeze()\n",
    "    print(sentenc_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "print(sentenc_out.view(sentenc_out.size(0), -1).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 1.0205e-02, -4.0110e-02, -3.1047e-02, -1.0309e-01,  6.1025e-02,\n",
      "          9.7053e-02,  2.3997e-02, -1.1006e-01, -7.9249e-02,  1.4116e-01,\n",
      "         -1.1850e-01,  7.6383e-03, -6.2079e-02, -5.2774e-02,  4.4817e-02,\n",
      "         -4.2933e-02,  1.0216e-02, -8.4887e-02,  5.0992e-02, -8.3613e-02,\n",
      "          2.5807e-02,  1.3175e-01,  2.9545e-02,  4.9353e-02, -2.2592e-02,\n",
      "         -6.6765e-02,  5.2205e-02,  1.1331e-01, -4.4384e-02,  9.8815e-03,\n",
      "          9.3066e-02, -2.8365e-02, -1.3375e-01, -1.3185e-02,  2.2988e-02,\n",
      "         -1.7301e-02,  1.2917e-01,  1.4210e-01, -9.3730e-02,  6.1013e-02,\n",
      "         -4.9668e-02, -5.0703e-02,  9.2445e-02, -9.9274e-02,  8.8130e-02,\n",
      "          6.3865e-02,  4.4382e-02, -7.2982e-02,  5.8645e-02, -6.2847e-02,\n",
      "         -1.4688e-02,  7.2641e-02, -3.0726e-02, -7.3274e-02,  1.0716e-01,\n",
      "          2.6822e-03, -1.5771e-01,  2.1897e-02,  3.5497e-02, -3.3932e-02,\n",
      "          6.1915e-02, -1.2432e-04, -9.6328e-02,  7.9967e-02]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "sentenc_out = sentenc_out.view(sentenc_out.size(0), -1)\n",
    "print(sentenc_out)\n",
    "print(sentenc_out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "sent_attention = nn.Linear(sent_encoder.hidden_size * 2, sent_encoder.hidden_size*2)\n",
    "u_s = nn.Linear(sent_encoder.hidden_size*2, 1, bias = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=64, out_features=64, bias=True)\n",
      "\n",
      "Linear(in_features=64, out_features=1, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(sent_attention)\n",
    "print()\n",
    "print(u_s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0028, -0.1099,  0.0240, -0.0190,  0.1199, -0.0506, -0.1016, -0.0022,\n",
      "         -0.0449, -0.0360, -0.0733, -0.0750,  0.0149,  0.0531, -0.0629, -0.0245,\n",
      "         -0.0260, -0.0269,  0.0742, -0.1036, -0.1236,  0.0388, -0.0268,  0.0266,\n",
      "          0.0487,  0.1230, -0.1080,  0.0132, -0.0438, -0.0700,  0.0354, -0.0659,\n",
      "         -0.0402,  0.0860,  0.1248, -0.1116, -0.1109,  0.0659, -0.0903, -0.0599,\n",
      "          0.0497,  0.1199, -0.0085, -0.0229,  0.1020,  0.0017, -0.0310, -0.1151,\n",
      "         -0.0694, -0.1083,  0.0136, -0.0538, -0.0688,  0.0474,  0.1114, -0.0669,\n",
      "         -0.0592,  0.0922,  0.0179,  0.0563,  0.1020,  0.1081, -0.0231,  0.1137],\n",
      "        [ 0.0028, -0.1099,  0.0240, -0.0190,  0.1199, -0.0506, -0.1016, -0.0022,\n",
      "         -0.0449, -0.0360, -0.0733, -0.0750,  0.0149,  0.0531, -0.0629, -0.0245,\n",
      "         -0.0260, -0.0269,  0.0742, -0.1036, -0.1236,  0.0388, -0.0268,  0.0266,\n",
      "          0.0487,  0.1230, -0.1080,  0.0132, -0.0438, -0.0700,  0.0354, -0.0659,\n",
      "         -0.0402,  0.0860,  0.1248, -0.1116, -0.1109,  0.0659, -0.0903, -0.0599,\n",
      "          0.0497,  0.1199, -0.0085, -0.0229,  0.1020,  0.0017, -0.0310, -0.1151,\n",
      "         -0.0694, -0.1083,  0.0136, -0.0538, -0.0688,  0.0474,  0.1114, -0.0669,\n",
      "         -0.0592,  0.0922,  0.0179,  0.0563,  0.1020,  0.1081, -0.0231,  0.1137],\n",
      "        [ 0.0028, -0.1099,  0.0240, -0.0190,  0.1199, -0.0506, -0.1016, -0.0022,\n",
      "         -0.0449, -0.0360, -0.0733, -0.0750,  0.0149,  0.0531, -0.0629, -0.0245,\n",
      "         -0.0260, -0.0269,  0.0742, -0.1036, -0.1236,  0.0388, -0.0268,  0.0266,\n",
      "          0.0487,  0.1230, -0.1080,  0.0132, -0.0438, -0.0700,  0.0354, -0.0659,\n",
      "         -0.0402,  0.0860,  0.1248, -0.1116, -0.1109,  0.0659, -0.0903, -0.0599,\n",
      "          0.0497,  0.1199, -0.0085, -0.0229,  0.1020,  0.0017, -0.0310, -0.1151,\n",
      "         -0.0694, -0.1083,  0.0136, -0.0538, -0.0688,  0.0474,  0.1114, -0.0669,\n",
      "         -0.0592,  0.0922,  0.0179,  0.0563,  0.1020,  0.1081, -0.0231,  0.1137],\n",
      "        [-0.1059, -0.1153, -0.0313, -0.0293,  0.1937, -0.0694, -0.0087, -0.0317,\n",
      "         -0.0096, -0.0123, -0.1155, -0.0565,  0.0605, -0.0088, -0.0950, -0.0279,\n",
      "         -0.0151,  0.0663,  0.0102, -0.0866, -0.1753, -0.0065, -0.0322, -0.0137,\n",
      "          0.0570,  0.1208, -0.1131, -0.0720,  0.0425, -0.0803,  0.0484,  0.0167,\n",
      "          0.0046,  0.1076,  0.1333, -0.0476, -0.1214,  0.0614, -0.0644, -0.0088,\n",
      "         -0.0069,  0.0941, -0.0124,  0.0887,  0.0577, -0.0159, -0.0962, -0.0761,\n",
      "         -0.0331, -0.1473, -0.0031,  0.0138, -0.1422,  0.0852,  0.1837, -0.0575,\n",
      "         -0.0697,  0.1078,  0.0541,  0.0755,  0.1299,  0.0776,  0.0253,  0.0552]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "x = sent_attention(sentenc_out)\n",
    "print(x)\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0028, -0.1094,  0.0240, -0.0190,  0.1193, -0.0505, -0.1012, -0.0022,\n",
      "         -0.0449, -0.0360, -0.0731, -0.0749,  0.0149,  0.0531, -0.0628, -0.0245,\n",
      "         -0.0260, -0.0269,  0.0741, -0.1032, -0.1230,  0.0388, -0.0268,  0.0266,\n",
      "          0.0487,  0.1224, -0.1076,  0.0132, -0.0438, -0.0699,  0.0353, -0.0658,\n",
      "         -0.0401,  0.0858,  0.1242, -0.1111, -0.1105,  0.0658, -0.0901, -0.0598,\n",
      "          0.0497,  0.1193, -0.0085, -0.0229,  0.1017,  0.0017, -0.0310, -0.1146,\n",
      "         -0.0693, -0.1078,  0.0136, -0.0538, -0.0687,  0.0473,  0.1109, -0.0668,\n",
      "         -0.0591,  0.0920,  0.0179,  0.0562,  0.1017,  0.1077, -0.0231,  0.1132],\n",
      "        [ 0.0028, -0.1094,  0.0240, -0.0190,  0.1193, -0.0505, -0.1012, -0.0022,\n",
      "         -0.0449, -0.0360, -0.0731, -0.0749,  0.0149,  0.0531, -0.0628, -0.0245,\n",
      "         -0.0260, -0.0269,  0.0741, -0.1032, -0.1230,  0.0388, -0.0268,  0.0266,\n",
      "          0.0487,  0.1224, -0.1076,  0.0132, -0.0438, -0.0699,  0.0353, -0.0658,\n",
      "         -0.0401,  0.0858,  0.1242, -0.1111, -0.1105,  0.0658, -0.0901, -0.0598,\n",
      "          0.0497,  0.1193, -0.0085, -0.0229,  0.1017,  0.0017, -0.0310, -0.1146,\n",
      "         -0.0693, -0.1078,  0.0136, -0.0538, -0.0687,  0.0473,  0.1109, -0.0668,\n",
      "         -0.0591,  0.0920,  0.0179,  0.0562,  0.1017,  0.1077, -0.0231,  0.1132],\n",
      "        [ 0.0028, -0.1094,  0.0240, -0.0190,  0.1193, -0.0505, -0.1012, -0.0022,\n",
      "         -0.0449, -0.0360, -0.0731, -0.0749,  0.0149,  0.0531, -0.0628, -0.0245,\n",
      "         -0.0260, -0.0269,  0.0741, -0.1032, -0.1230,  0.0388, -0.0268,  0.0266,\n",
      "          0.0487,  0.1224, -0.1076,  0.0132, -0.0438, -0.0699,  0.0353, -0.0658,\n",
      "         -0.0401,  0.0858,  0.1242, -0.1111, -0.1105,  0.0658, -0.0901, -0.0598,\n",
      "          0.0497,  0.1193, -0.0085, -0.0229,  0.1017,  0.0017, -0.0310, -0.1146,\n",
      "         -0.0693, -0.1078,  0.0136, -0.0538, -0.0687,  0.0473,  0.1109, -0.0668,\n",
      "         -0.0591,  0.0920,  0.0179,  0.0562,  0.1017,  0.1077, -0.0231,  0.1132],\n",
      "        [-0.1055, -0.1147, -0.0313, -0.0293,  0.1913, -0.0693, -0.0087, -0.0317,\n",
      "         -0.0096, -0.0123, -0.1150, -0.0565,  0.0604, -0.0088, -0.0947, -0.0279,\n",
      "         -0.0151,  0.0662,  0.0102, -0.0864, -0.1735, -0.0065, -0.0322, -0.0137,\n",
      "          0.0569,  0.1202, -0.1126, -0.0719,  0.0425, -0.0801,  0.0484,  0.0167,\n",
      "          0.0046,  0.1072,  0.1325, -0.0476, -0.1209,  0.0613, -0.0643, -0.0088,\n",
      "         -0.0069,  0.0938, -0.0124,  0.0885,  0.0577, -0.0159, -0.0959, -0.0759,\n",
      "         -0.0331, -0.1463, -0.0031,  0.0138, -0.1412,  0.0850,  0.1817, -0.0574,\n",
      "         -0.0696,  0.1074,  0.0541,  0.0754,  0.1291,  0.0775,  0.0253,  0.0551]],\n",
      "       grad_fn=<TanhBackward>)\n",
      "torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "u_sent = torch.tanh(x)\n",
    "print(u_sent)\n",
    "print(u_sent.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0157],\n",
      "        [0.0157],\n",
      "        [0.0157],\n",
      "        [0.0575]], grad_fn=<MmBackward>)\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "y = u_s(u_sent)\n",
    "print(y)\n",
    "print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentence weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2474],\n",
      "        [0.2474],\n",
      "        [0.2474],\n",
      "        [0.2579]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "sent_weights = softmax(y)\n",
    "print(sent_weights)\n",
    "print(sent_weights.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(sent_weights.sum(dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Document vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0007, -0.0271,  0.0059, -0.0047,  0.0295, -0.0125, -0.0250, -0.0006,\n",
      "         -0.0111, -0.0089, -0.0181, -0.0185,  0.0037,  0.0131, -0.0155, -0.0061,\n",
      "         -0.0064, -0.0067,  0.0183, -0.0255, -0.0304,  0.0096, -0.0066,  0.0066,\n",
      "          0.0120,  0.0303, -0.0266,  0.0033, -0.0108, -0.0173,  0.0087, -0.0163,\n",
      "         -0.0099,  0.0212,  0.0307, -0.0275, -0.0273,  0.0163, -0.0223, -0.0148,\n",
      "          0.0123,  0.0295, -0.0021, -0.0057,  0.0251,  0.0004, -0.0077, -0.0284,\n",
      "         -0.0172, -0.0267,  0.0034, -0.0133, -0.0170,  0.0117,  0.0274, -0.0165,\n",
      "         -0.0146,  0.0228,  0.0044,  0.0139,  0.0252,  0.0266, -0.0057,  0.0280],\n",
      "        [ 0.0007, -0.0271,  0.0059, -0.0047,  0.0295, -0.0125, -0.0250, -0.0006,\n",
      "         -0.0111, -0.0089, -0.0181, -0.0185,  0.0037,  0.0131, -0.0155, -0.0061,\n",
      "         -0.0064, -0.0067,  0.0183, -0.0255, -0.0304,  0.0096, -0.0066,  0.0066,\n",
      "          0.0120,  0.0303, -0.0266,  0.0033, -0.0108, -0.0173,  0.0087, -0.0163,\n",
      "         -0.0099,  0.0212,  0.0307, -0.0275, -0.0273,  0.0163, -0.0223, -0.0148,\n",
      "          0.0123,  0.0295, -0.0021, -0.0057,  0.0251,  0.0004, -0.0077, -0.0284,\n",
      "         -0.0172, -0.0267,  0.0034, -0.0133, -0.0170,  0.0117,  0.0274, -0.0165,\n",
      "         -0.0146,  0.0228,  0.0044,  0.0139,  0.0252,  0.0266, -0.0057,  0.0280],\n",
      "        [ 0.0007, -0.0271,  0.0059, -0.0047,  0.0295, -0.0125, -0.0250, -0.0006,\n",
      "         -0.0111, -0.0089, -0.0181, -0.0185,  0.0037,  0.0131, -0.0155, -0.0061,\n",
      "         -0.0064, -0.0067,  0.0183, -0.0255, -0.0304,  0.0096, -0.0066,  0.0066,\n",
      "          0.0120,  0.0303, -0.0266,  0.0033, -0.0108, -0.0173,  0.0087, -0.0163,\n",
      "         -0.0099,  0.0212,  0.0307, -0.0275, -0.0273,  0.0163, -0.0223, -0.0148,\n",
      "          0.0123,  0.0295, -0.0021, -0.0057,  0.0251,  0.0004, -0.0077, -0.0284,\n",
      "         -0.0172, -0.0267,  0.0034, -0.0133, -0.0170,  0.0117,  0.0274, -0.0165,\n",
      "         -0.0146,  0.0228,  0.0044,  0.0139,  0.0252,  0.0266, -0.0057,  0.0280],\n",
      "        [-0.0272, -0.0296, -0.0081, -0.0076,  0.0493, -0.0179, -0.0022, -0.0082,\n",
      "         -0.0025, -0.0032, -0.0297, -0.0146,  0.0156, -0.0023, -0.0244, -0.0072,\n",
      "         -0.0039,  0.0171,  0.0026, -0.0223, -0.0448, -0.0017, -0.0083, -0.0035,\n",
      "          0.0147,  0.0310, -0.0291, -0.0185,  0.0110, -0.0207,  0.0125,  0.0043,\n",
      "          0.0012,  0.0276,  0.0342, -0.0123, -0.0312,  0.0158, -0.0166, -0.0023,\n",
      "         -0.0018,  0.0242, -0.0032,  0.0228,  0.0149, -0.0041, -0.0247, -0.0196,\n",
      "         -0.0085, -0.0377, -0.0008,  0.0035, -0.0364,  0.0219,  0.0469, -0.0148,\n",
      "         -0.0180,  0.0277,  0.0139,  0.0194,  0.0333,  0.0200,  0.0065,  0.0142]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([4, 64])\n",
      "\n",
      "tensor([-0.0251, -0.1108,  0.0097, -0.0217,  0.1379, -0.0554, -0.0774, -0.0098,\n",
      "        -0.0358, -0.0299, -0.0839, -0.0701,  0.0267,  0.0371, -0.0710, -0.0254,\n",
      "        -0.0232, -0.0029,  0.0576, -0.0989, -0.1360,  0.0271, -0.0282,  0.0162,\n",
      "         0.0508,  0.1218, -0.1089, -0.0087, -0.0216, -0.0725,  0.0387, -0.0445,\n",
      "        -0.0286,  0.0913,  0.1263, -0.0947, -0.1132,  0.0646, -0.0834, -0.0467,\n",
      "         0.0351,  0.1127, -0.0095,  0.0058,  0.0903, -0.0028, -0.0478, -0.1046,\n",
      "        -0.0600, -0.1178,  0.0093, -0.0364, -0.0874,  0.0570,  0.1292, -0.0644,\n",
      "        -0.0618,  0.0960,  0.0272,  0.0611,  0.1088,  0.0999, -0.0106,  0.0982],\n",
      "       grad_fn=<SumBackward1>)\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "doc_summ_vector = (u_sent * sent_weights)\n",
    "print(doc_summ_vector)\n",
    "print(doc_summ_vector.shape)\n",
    "print()\n",
    "doc_summ_vector = doc_summ_vector.sum(axis=0)\n",
    "print(doc_summ_vector)\n",
    "print(doc_summ_vector.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "z= (u_sent * sent_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final Layer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=64, out_features=6, bias=True)\n"
     ]
    }
   ],
   "source": [
    "dense_out = nn.Linear(sent_encoder.hidden_size*2, NUM_CLASSES)\n",
    "print(dense_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1060,  0.0195, -0.0629,  0.0993, -0.1410, -0.0569],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "out = dense_out(doc_summ_vector)\n",
    "print(out)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}